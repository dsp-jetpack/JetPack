# Copyright (c) 2015-2021 Dell Inc. or its subsidiaries.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

###########################################################################
#                                                                         #
# Copy and rename this file for your own stamp.                           #
# Review ALL settings below, pay particular attention to paths/ip's etc.. #
#                                                                         #
###########################################################################


[Network Settings]

# Nova public network details
public_api_network=192.168.190.0/24
public_api_vlanid=190
public_api_gateway=192.168.190.1
public_api_netmask=255.255.255.0
public_api_allocation_pool_start=192.168.190.121
public_api_allocation_pool_end=192.168.190.250
name_server=8.8.8.8

# Private API network details
private_api_network=192.168.140.0/24
private_api_vlanid=140
private_api_gateway=192.168.140.1
private_api_netmask=255.255.255.0
private_api_allocation_pool_start=192.168.140.121
private_api_allocation_pool_end=192.168.140.250

# Storage network details
storage_network=192.168.170.0/24
storage_gateway=192.168.170.1
storage_vlanid=170
storage_netmask=255.255.255.0
storage_allocation_pool_start=192.168.170.125
storage_allocation_pool_end=192.168.170.250

# Provisioning network details
provisioning_network=192.168.120.0/24
provisioning_vlanid=120
provisioning_netmask=255.255.255.0
provisioning_gateway=192.168.120.1
provisioning_net_dhcp_start=192.168.120.121
provisioning_net_dhcp_end=192.168.120.250
discovery_ip_range=192.168.120.21,192.168.120.120

# Storage cluster network details
storage_cluster_network=192.168.180.0/24
storage_cluster_vlanid=180
storage_cluster_gateway=192.168.180.1
storage_cluster_allocation_pool_start=192.168.180.121
storage_cluster_allocation_pool_end=192.168.180.250

# Management network details
# Make sure the SAH node idrac ip defined in the .properties
# is NOT within the allocation pool below.
management_network=192.168.110.0/24
management_vlanid=110
management_netmask=255.255.255.0
management_gateway=192.168.110.1
management_allocation_pool_start=192.168.110.100
management_allocation_pool_end=192.168.110.199

# Tenant network details
# Not used unless you wish to configure Generic Routing Encapsulation (GRE) networks.
tenant_tunnel_network=192.168.130.0/24
tenant_tunnel_gateway=192.168.130.1
tenant_tunnel_network_allocation_pool_start=192.168.130.121
tenant_tunnel_network_allocation_pool_end=192.168.130.250
tenant_tunnel_network_vlanid=130

# Nova Private network details
tenant_vlan_range=201:250


[MTU Settings]

# The mtu_selection setting defines whether to use the "global" or "per_network" option.
# If the mtu_selection is defined as "global", the mtu value for all networks will be set to the value provided in mtu_size_global_default. The supported value must be within the range of 1500-9000.
# If "per_network" mtu_selection is defined, the user should provide an mtu value for each network in the range of 1500-9000.
# For "public_api_network_mtu" and "floating_ip_network_mtu" networks, mtu sizes greater than 1500 is only supported if jumbo frames are enabled on upstream routers.
mtu_selection=global
mtu_size_global_default=1500
public_api_network_mtu=1500
floating_ip_network_mtu=1500
private_api_network_mtu=1500
tenant_network_mtu=1500
storage_cluster_network_mtu=1500
storage_network_mtu=1500
tenant_tunnel_network_mtu=1500

[Vips Settings]

# Use static VIPs ip addresses for the overcloud
use_static_vips=true

# The following VIP settings apply if the above use_static_vips is enabled.

# VIP for the redis service on the Private API api network
# Note that this IP must lie outside the private_api_allocation_pool_start/end
# range
redis_vip=192.168.140.251

# VIP for the provisioning network
# Note that this IP must lie outside the provisioning_net_dhcp_start/end range
provisioning_vip=192.168.120.251

# VIP for the Private API network
# Note that this IP must lie outside the private_api_allocation_pool_start/end
# range
private_api_vip=192.168.140.252

# VIP for the Public API network
# Note that this IP must lie outside the public_api_allocation_pool_start/end
# range
public_api_vip=192.168.190.251

# VIP for the Storage network
# Note that this IP must lie outside the storage_allocation_pool_start/end
# range
storage_vip=192.168.170.251

# VIP for the Storage cluster network
# The Storage Clustering network is not connected to the controller nodes,
# so the VIP for this network must be mapped to the provisioning network
# Note that this IP must lie outside the provisioning_net_dhcp_start/end range
storage_cluster_vip=192.168.120.252


[Subscription Manager Settings]

# Subscription Manager account info for registering Red Hat subscriptions
subscription_manager_user=xxxxxxx
subscription_manager_password=xxxxxxxxxxxxxx

# The following pool IDs provide different collections of repositories.
# Each is labeled with possible subscription names.

# Red Hat Enterprise Linux (Physical Node)
subscription_manager_pool_sah=xxxxxxxxxxxxxxxxxxxxxxxxxxxx44f5

# Red Hat Enterprise Linux OpenStack Platform (Virtual Node)
subscription_manager_pool_vm_rhel=xxxxxxxxxxxxxxxxxxxxxxxxxxxx454a

subscription_check_retries=20

# Red Hat Satellite Settings
use_satellite=false
satellite_ip=xxx.xxx.xxx.xx
satellite_hostname=samplehost.domain
satellite_org=sample_org
satellite_activation_key=sample_key

# Container images can be pulled from satellite rather than cdn if desired
pull_containers_from_satellite=false
# Satellite containers prefix, as per https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html/director_installation_and_usage/preparing-for-director-installation#preparing-a-satellite-server-for-container-images
containers_prefix=organization-product-

[Nodes Nics and Bonding Settings]

sah_bond_opts=mode=802.3ad,miimon=100,xmit_hash_policy=layer3+4,lacp_rate=1

# NIC environment file
#       1. To enable standard XSP profile or standard CSP profile (NUMA and HugePages):
#          For 5 ports, choose 5_port/nic_environment.yaml.
#          For 4 ports, choose 4_port/nic_environment.yaml.
#       2. To enable SR-IOV with 6 nic ports, choose sriov_6_port/nic_environment.yaml.
#          For 7 ports, choose sriov_7_port/nic_enviornment.yaml.
#          For 8 ports, choose sriov_8_port/nic_environment.yaml.
#          For 9 ports, choose sriov_9_port/nic_environment.yaml.
#       3. To enable SR-IOV with Hardware Offload with 6 nic ports, choose sriov_6_port_offload/nic_environment.yaml.
#          For 7 ports, choose sriov_7_port_offload/nic_enviornment.yaml.
#          For 8 ports, choose sriov_8_port_offload/nic_environment.yaml.
#          For 9 ports, choose sriov_9_port_offload/nic_environment.yaml.
#       4. To enable OVS-DPDK with 6 nic ports, choose ovs-dpdk_6_port/nic_environment.yaml.
#          For 7 ports, choose ovs-dpdk_7_port/nic_enviornment.yaml.
#          For 8 ports, choose ovs-dpdk_8_port/nic_enviornment.yaml.
#          For 9 ports, choose ovs-dpdk_9_port/nic_enviornment.yaml.
#       5. To enable OVS-DPDK (2-ports) and SR-IOV (2-ports), choose ovs-dpdk_sriov_9_port/nic_environment.yaml.
#          For 8 ports OVS-DPDK (2-ports) and SR-IOV (2-ports), choose ovs-dpdk_sriov_8_port/nic_environment.yaml.
#       6. To enable DVR, for Storage and Floating networks that share a single bond, choose 5_port/nic_environment.yaml.
#          If a separate bond is required for Floating network on compute nodes, choose dvr_7_port/nic_environment.yaml.
nic_env_file=4_port/nic_environment.yaml

# In case of ovs-dpdk_7_port, ovs-dpdk_9_port or ovs-dpdk_sriov_9_port configuration
# Choose the appropriate driver for the NICs being used. For Intel, use "vfio-pci"
# and for Mellanox use "mlx5_core"
# The Following line should be commented out if ovs_dpdk_enable is set to false
#HostNicDriver=mlx5_core

# Interfaces and bonding options per node type.
# When using any 4 port NIC configuration, comment out or delete the
# ControllerProvisioningInterface line below.
#ControllerProvisioningInterface=em3
ControllerBond0Interface1=em1
ControllerBond0Interface2=p1p1
ControllerBond1Interface1=em2
ControllerBond1Interface2=p1p2
ControllerBondInterfaceOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1

# When using any 4 port NIC configuration, comment out or delete the
# ComputeProvisioningInterface line below.
#ComputeProvisioningInterface=em3
ComputeBond0Interface1=em1
ComputeBond0Interface2=p1p1
ComputeBond1Interface1=em2
ComputeBond1Interface2=p1p2
ComputeBondInterfaceOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1

# When using any 4 port NIC configuration, comment out or delete the
# StorageProvisioningInterface line below.
#StorageProvisioningInterface=em3
StorageBond0Interface1=em1
StorageBond0Interface2=p2p1
StorageBond1Interface1=em2
StorageBond1Interface2=p2p2
StorageBondInterfaceOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1

# To enable standalone OVS-DPDK, two or four interfaces should be used.
# For two interfaces, uncomment 'ComputeOvsDpdkInterface1', 'ComputeOvsDpdkInterface2' and 'BondInterfaceOvsOption'.
# For four interfaces, uncomment all four interfaces and 'BondInterfaceOvsOption'.
# The following lines should be commented out  if ovs_dpdk_enable is set to false
#ComputeOvsDpdkInterface1=p2p1
#ComputeOvsDpdkInterface2=p3p1
#ComputeOvsDpdkInterface3=p2p2
#ComputeOvsDpdkInterface4=p3p2
#BondInterfaceOvsOptions=bond_mode=balance-tcp lacp=active

# To enable standalone SR-IOV, two or four interfaces should be used without bonding options.
# For two interfaces, uncomment 'ComputeSriovInterface1' and 'ComputeSriovInterface2'.
# For four interfaces, uncomment all four interfaces.
# To enable Sriov ovs-hw-offload with bond/vf-lag, smart_nic should be enabled and bonding options uncommented
# ComputeSriovInterface1 & ComputeSriovInterface2 form bond-pf0 and these interfaces should be from single nic
# ComputeSriovInterface3 & ComputeSriovInterface4 form bond-pf1 and these interfaces should be from single nic
# Ovs will not be offloaded if bonded interfaces are not from same nic.
# Following lines should be commented out if sriov_enable is set to false
#ComputeSriovInterface1=p1p1
#ComputeSriovInterface2=p1p2
#ComputeSriovInterface3=p4p1
#ComputeSriovInterface4=p4p2
#BondInterfaceSriovOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1

# To enable SR-IOV and OVS-DPDK, four interfaces should be used.
# Following lines should be uncommented if both sriov_enable and ovs-dpdk_enable are set to true.
#ComputeSriovInterface1=p1p1
#ComputeSriovInterface2=p4p1
#ComputeOvsDpdkInterface1=p2p1
#ComputeOvsDpdkInterface2=p3p1
#BondInterfaceOvsOptions=bond_mode=balance-tcp lacp=active

# To enable DVR with a separate bond for Floating network on compute nodes, two interfaces should be used.
# To use the two interfaces, uncomment 'ComputeBond2Interface1' and 'ComputeBond2Interface2'.
# Following lines should be commented out if dvr_enable is set to false.
#ComputeBond2Interface1: p3p1
#ComputeBond2Interface2: p1p2

[Dell NFV Settings]

# Enter value of enable_hpg as true/false for HugePages
hpg_enable=false
# User should give this parameter in same format.
# Supported values for hpg_size(Size of hugepages) is 2MB and 1 GB.
# The number of hugepages will be calculated dynamically.
hpg_size=1GB

# Enter value of enable_numa as true/false for NUMA
numa_enable=false

# OVS-DPDK Settings
# Set the following option to true/false
ovs_dpdk_enable=false

# Enter number of cores you want to reserve for Host OS
# Supported values are 2,4,6,8
numa_hostos_cpu_count=4

# SRIOV Settings
# Set the following option to true/false for SRIOV
sriov_enable=false

# Set the following option to true/false for SRIOV Offload
# SRIOV hardware offload is only supported with smart NICs
# i.e. Mellanox ConnectX-5 and sriov_6_port configuration
smart_nic=false

# Enter the number of VFs you want to create per port
# Supported values are between 1-64
sriov_vf_count=64

# Set the following option to true/false for DVR
dvr_enable=false

# Set the following option to true/false for Barbican
barbican_enable=false

# Set the following option to true/false for Octavia
octavia_enable=false
# Set the octavia_generate_certs parameter to false to
# provide your own certificates and keys to octavia
# If octavia_generate_certs is set to false, provide a custom
# environment file containing your certificates and keys in yaml format
octavia_generate_certs=true
certificate_keys_path=/pathto/octavia-environment.yaml

[IPMI credentials Settings]

# DRAC credentials with IPMI privilege for the SAH node
sah_ipmi_user=root
sah_ipmi_password=xxxxxxx

# DRAC credentials with IPMI privilege for the overcloud nodes
ipmi_user=root
ipmi_password=xxxxxxx
# A password to change to on overcloud nodes if desired
new_ipmi_password=


[Deployment Settings]

# Valid values are csp, xsp and custom.
profile=xsp

# This pathname must be the full path to the properties file which
# describes the cluster. You should copy *this* sample settings file
# (sample.ini) and the sample properties file (sample.properties) to
# another directory, and customize them for your cluster. Then use the
# path to your customized properties file here.
cluster_nodes_configuration_file=/root/acme.properties

# User for the undercloud/overcloud installation
director_install_user=osp_admin
director_install_user_password=xxxxxxx

#The IP address or hostname defined for director Admin API endpoints over SSL/TLS.
#The director configuration attaches the IP address to the director software
#bridge as a routed IP address, which uses the /32 netmask.
#This IP should be from the provisioning network e.g 192.168.120.0/24
undercloud_admin_host=192.168.120.253

#The IP address or hostname defined for director Public API endpoints over SSL/TLS.
#The director configuration attaches the IP address to the director software
#bridge as a routed IP address, which uses the /32 netmask.
#This IP should be from the provisioning network e.g 192.168.120.0/24
undercloud_public_host=192.168.120.254

# Name of the Overcloud.
# The nodes hostnames will be prepended with the given name and a dash
overcloud_name=overcloud

# Domain name for the cluster (i.e., mycluster.lab)
domain=domain.net

# Optional : root passord for the overcloud nodes, no password is set if left empty
overcloud_nodes_pwd=

# , separated list of ntp servers
ntp_servers=0.centos.pool.ntp.org
time_zone=America/Chicago

# Use static ip adresses for the overcloud nodes if set to true (ips need to be defined in the .properties)
# Use dhcp if set to false (ips not required in the .properties)
overcloud_static_ips=true

# Set to true to enable cinder backend of Ceph for storage.
enable_rbd_backend=true

# Set to true to enable Nova usage of Ceph for ephemeral storage.
# If set to false, Nova uses the storage local to the compute.
enable_rbd_nova_backend=true

# Set the glance backend. Values are file, rbd, swift or cinder
glance_backend=rbd

# Set to false to disable fencing
enable_fencing=true

# Set to false to disable the ceph dashboard deployment
enable_dashboard=true


[Storage back-end Settings]

# Storage Backends: Ceph, Dell EMC Unity, Dell EMC PowerMax, Dell EMC Storage Center
#       1. To enable Ceph as the storage backend in the solution:
#            For Cinder/Block Storage:
#              set 'enable_rbd_backend=true'
#              set 'enable_dellsc_backend=false'
#              set 'enable_unity_backend=false'
#              set 'enable_powermax_backend=false'
#            For Glance/Image Storage:
#              set 'glance_backend=rbd'
#            For Nova Ephemeral Storage,
#              set 'enable_rbd_nova_backend=true'
#       2. To enable Dell EMC Unity as the storage backend in the solution:
#            For Cinder/Block Storage:
#              set 'enable_rbd_backend=false'
#              set 'enable_dellsc_backend=false'
#              set 'enable_unity_backend=true'
#              set 'enable_powermax_backend'=false'
#            For Manila/Share Storage:
#              set 'enable_unity_manila_backend=true'
#            For Glance/Image Service:
#              set 'glance_backend=cinder'
#            For Nova Ephemeral Storage, false defaults to local
#              set 'enable_rbd_nova_backend=false'
#       3. To enable Dell EMC Powermax as the storage backend in the solution:
#            For Cinder/Block Storage:
#              set 'enable_rbd_backend=false'
#              set 'enable_dellsc_backend=false'
#              set 'enable_unity_backend=false'
#              set 'enable_powermax_backend=true'
#            For Glance/Image Service:
#              set 'glance_backend=cinder'
#            For Nova Ephemeral Storage, false defaults to local
#              set 'enable_rbd_nova_backend=false'
#       4. To enable Dell EMC Storage Center as the storage backend in the solution:
#            For Cinder/Block Storage:
#              set 'enable_rbd_backend=false'
#              set 'enable_dellsc_backend=true'
#              set 'enable_unity_backend=false'
#              set 'enable_powermax_backend=false'
#            For Glance/Image Service:
#              set 'glance_backend=cinder'
#            For Nova Ephemeral Storage, false defaults to local
#              set 'enable_rbd_nova_backend=false'
#       5. For a multi-backend deployment, enable/disable selected backends


# Dell Storage Center cinder parameters.
enable_dellsc_backend=false
dellsc_backend_name=CHANGEME
dellsc_api_port=3033
dellsc_iscsi_ip_address=CHANGEME
dellsc_iscsi_port=3260
dellsc_san_ip=CHANGEME
dellsc_san_login=CHANGEME
dellsc_san_password=CHANGEME
dellsc_ssn=CHANGEME
dellsc_server_folder=cmpl_iscsi_servers
dellsc_volume_folder=cmpl_iscsi_volumes
dellsc_second_san_ip=CHANGEME
dellsc_second_san_login=CHANGEME
dellsc_second_san_password=CHANGEME
dellsc_second_api_port=3033
dellsc_excluded_domain_ip=
dellsc_multipath_xref=true

#
# Unity cinder parameters.
#
enable_unity_backend=false
# The Dell EMC Unity Container is published in the RH Container Catalog
# registry.connect.redhat.com
# By default, this version of the image of the RHOSP release is pulled.
# Change to a different version if a specific image is required.
cinder_unity_container_version=16.1-2
unity_backend_name=CHANGEME
unity_san_ip=CHANGEME
unity_san_login=CHANGEME
unity_san_password=CHANGEME
#Valid Protocols are iSCSI or FC
unity_storage_protocol=iSCSI
unity_io_ports=CHANGEME
unity_storage_pool_names=CHANGEME

#
# Unity manila parameters.
#
enable_unity_manila_backend=false
# The Dell EMC Unity Container is published in the RH Container Catalog
# registry.connect.redhat.com
# By default, this version of the image of the RHOSP release is pulled.
# Change to a different version if a specific image is required.
manila_unity_container_version=16.1-2
manila_unity_driver_handles_share_servers=true
manila_unity_nas_login=CHANGEME
manila_unity_nas_password=CHANGEME
manila_unity_nas_server=CHANGEME
manila_unity_server_meta_pool=CHANGEME
manila_unity_share_data_pools=CHANGEME
manila_unity_ethernet_ports=CHANGEME
manila_unity_ssl_cert_verify=false
manila_unity_ssl_cert_path=

#
#Powermax cinder parameters.
#
enable_powermax_backend=false
powermax_backend_name=CHANGEME
powermax_san_ip=CHANGEME
powermax_san_login=CHANGEME
powermax_san_password=CHANGEME
# Valid protocol values are iSCSI or FC
powermax_protocol=iSCSI
powermax_array=CHANGEME
powermax_port_groups=CHANGEME
powermax_srp=CHANGEME

#
# PowerMax manila parameters.
#
enable_powermax_manila_backend=false
manila_powermax_driver_handles_share_servers=true
manila_powermax_nas_login=CHANGEME
manila_powermax_nas_password=CHANGEME
manila_powermax_nas_server=CHANGEME
manila_powermax_server_container=CHANGEME
manila_powermax_share_data_pools=CHANGEME
manila_powermax_ethernet_ports=CHANGEME

#
# PowerFlex cinder parameters
#
powerflex_presentation_server_rpm=EMC-ScaleIO-mgmt-server-3.5-0.365.noarch.rpm
powerflex_gateway_rpm=EMC-ScaleIO-gateway-3.5-0.468.x86_64.rpm
enable_powerflex_backend=false
enable_powerflex_mgmt=false
powerflex_san_login=CHANGEME
powerflex_san_password=CHANGEME
powerflex_storage_pools=CHANGEME:CHANGEME


[Sanity Test Settings]

# If you want the sanity script to run on deployment completion (Appendix C, etc.), you may do so.
run_sanity=false

floating_ip_network=192.168.191.0/24
floating_ip_network_start_ip=192.168.191.20
floating_ip_network_end_ip=192.168.191.59
floating_ip_network_gateway=192.168.191.1
floating_ip_network_vlan=191
sanity_tenant_network=192.168.201.0/24
sanity_user_password=s@n1ty
sanity_user_email=someone@somewhere.com
sanity_key_name=sanity

# The number of instances to spin up in nova.
# Note that this will be limited by the instance quota in OpenStack, which is
# 10 by default.
sanity_number_instances=1
sanity_image_url=http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2

# vlan-aware specific parameters
# Set the following option to true/false for vlan aware sanity test. If set to true, one additional
# instance will be created for vlan-aware testing.
vlan_aware_sanity=false
# address of vlan-network where subport is attached
sanity_vlantest_network=192.168.216.0/24

# storage network for manila shares
share_storage_network=100.82.47.0/24
share_storage_network_start_ip=100.82.47.70
share_storage_network_end_ip=100.82.47.73
share_storage_network_gateway=100.82.47.1
share_storage_network_vlan=47
share_storage_network_name=share_storage
share_storage_subnet_name=share_storage_sub

[Tempest Settings]

# If you want to run Tempest post-deployment, you may do so.
# Note: The sanity test script must be run first to create networks for Tempest.
run_tempest=false

# The flag tempest_smoke_only is dependent on run_tempest=true for
# tempest_smoke_only to have any effect.
# If run_tempest=true and tempest_smoke_only=true, only a subset of tempest tests
# tagged as smoke tests will be executed.  This set of tests is useful for validating
# basic OpenStack functionality.
tempest_smoke_only=true

# The tempest workspace directory which will be created in the user's home
# directory, and initialized when tempest is configured.
tempest_workspace=mytempest

[Advanced Settings]

#The following settings should typically only be used by developers

# Only developers should set to false.
enable_version_locking=false

# The list of RHSM repositories to enable to access the product.  Repos should
# be comma separated.
# Note that this parameter is defaulted as shown below when commented out or
# not specified.  It should not be necessary to change it from the default in
# most cases.
rhsm_repos=rhel-8-for-x86_64-baseos-eus-rpms,rhel-8-for-x86_64-appstream-eus-rpms,rhel-8-for-x86_64-highavailability-eus-rpms,ansible-2.9-for-rhel-8-x86_64-rpms,advanced-virt-for-rhel-8-x86_64-rpms,satellite-tools-6.5-for-rhel-8-x86_64-rpms,openstack-16.1-for-rhel-8-x86_64-rpms,fast-datapath-for-rhel-8-x86_64-rpms

# Option below is to use a custom instack.json and skip discover_nodes
use_custom_instack_json=false
custom_instack_json=n/a

# Indicates if the deploy-overcloud.py script should be run in debug mode
deploy_overcloud_debug=false

use_internal_repo=false
# Semi-colon ( ; ) separated list of internal repos to use, if needed.
internal_repos_locations=CHANGEME_INTERNAL_REPO_URL

cloud_repo_dir=/root/JetPack
rhel_iso=/root/rhel82.iso

# Overcloud deployment timeout value - default is 180mns, but can be tweaked here if required.
overcloud_deploy_timeout=300

# Default driver is DRAC.
use_ipmi_driver=false

# Default introspection method is out-of-band.
# Note that out-of-band introspection is only supported by the DRAC driver.  If
# use_ipmi_driver is set to "true" above then in-band introspection will be
# used regardless of the value below.
use_in_band_introspection=false

# RDO cloud images
# Available to download @ https://access.redhat.com/downloads/content/191/ver=8/rhel---7/8/x86_64/product-software
discovery_ram_disk_image=/pathto/discovery-ramdisk-7.1.0-39.tar
overcloud_image=/pathto/overcloud-full-7.1.0-39.tar
# if option below is enabled, images will be pulled fom the cdn (and the above x2 settings ignored)
pull_images_from_cdn=true

# Occasionally there can be problems with Subscription Manager
# and a node may be properly registered yet "subscription manager status"
# will return "Unknown"  which will cause checks to fail.
# Setting this to false will skip SM checks to get around this issue.
verify_rhsm_status=true

# Generic node type definitions to initially be used for edge but this feature
# will eventually be leveraged to define a wide variety of node type
# configurations.
# Where ini files and ConfigParser only allow for flat ini
# file structure, we use the following convention to support
# multiple subnets, initially for edge support, in a deployment:
# edge_sites = edge-compute-denver, edge-compute-boston
edge_sites=
# When deploy_edge_sites is set to true jetpack will deploy all the
# edge sites declared in edge_sites above after the overcloud has
# been deplopyed.  This setting is only used during a full deployment or
# --overcloud_only deployment.
deploy_edge_sites=true

# For edge sites you must define the edge site stanza name using the
# convention where the edge site stanza name is the same
# as the edge site name (see edge_sites attribute above):
# [edge-compute-denver]
# cidr = 192.168.121.0/24
# mgmt_cidr = 192.168.111.0/24
# dhcp_start = 192.168.121.121
# dhcp_end = 192.168.121.250
# inspection_iprange = 192.168.121.51,192.168.121.120
# gateway = 192.168.121.1
# nic_port_count = 5
# When using any 4 port NIC configuration, comment out or delete the
# ProvisioningInterface line below.
# ProvisioningInterface=eno1
# Bond0Interface1=ens1f0
# Bond0Interface2=ens3f0
# Bond1Interface1=ens1f1
# Bond1Interface2=ens3f1
# private_api_vlanid=141
# private_api_network=192.168.141.0/24
# private_api_gateway=192.168.141.1
# private_api_allocation_pool_start=192.168.141.20
# private_api_allocation_pool_end=192.168.141.60
# tenant_network=192.168.131.0/24
# tenant_gateway=192.168.131.1
# tenant_vlanid=131
# tenant_allocation_pool_start=192.168.131.20
# tenant_allocation_pool_end=192.168.131.60
# storage_network=192.168.171.0/24
# storage_gateway=192.168.171.1
# storage_vlanid=171
# storage_allocation_pool_start=192.168.171.20
# storage_allocation_pool_end=192.168.171.60
# external_network=100.67.62.128/26
# external_vlanid=623
# external_gateway=100.67.62.129
# external_allocation_pool_start=100.67.62.140
# external_allocation_pool_end=100.67.62.150
# NFV site settings
# nfv_type must be one of dpdk, sriov or both
# nfv_type=sriov
# nfv_mtu=1500
# the number of NfvInterfaceN attributes is dependent on the number of Ports
# If ports are <= 7 you only need two NFVInterfaces > 7 implies four
# The order is important, pairs need to be across nics.
# nfv_interfaces=ens4f0,ens5f0,ens4f1,ens5f1
# HostNicDriver=vfio-pci
# BondInterfaceOvsOptions=bond_mode=balance-tcp lacp=active
# BondInterfaceSriovOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1
# NumDpdkInterfaceRxQueues = 1
# hpg_enable=true
# hpg_size=1GB
# numa_enable=true
# numa_hostos_cpu_count=4
# sriov_vf_count=4

# [edge-compute-boston]
# cidr = 192.168.122.0/24
# mgmt_cidr = 192.168.112.0/24
# dhcp_start = 192.168.122.121
# dhcp_end = 192.168.122.250
# inspection_iprange = 192.168.122.51,192.168.122.120
# gateway = 192.168.122.1

[PowerFlex Settings]
# Method to retrieve RPMs before Installation begins
# Can be either local or external
powerflex_rpms_method=local

# Used to override default RPMs path.
powerflex_rpms_path='/opt/dellemc/powerflex/rpms'

# Name of the PowerFlex OS cluster
# The PowerFlex cluster name must be at least 1 character and contains only letters and numbers
powerflex_cluster_name=CHANGEME

# Name of the  PowerFlex OS Protection Domain
# A single protection is defined for the whole system.
powerflex_protection_domain=CHANGEME

# Name of the  PowerFlex OS Storage Pool
# A single storage pool is defined where all disks of the same type and speed will reside.
powerflex_storage_pool=CHANGEME

# PowerFlex OS Cluster Configuration
# Only 3-node cluster config is supported at this time
# 5 nodes model is not supported yet.
powerflex_cluster_config=3_node

# Interface used for Management access. Must be accessible from external. (Optional)
powerflex_mgmt_interface=CHANGEME

# Current Interface for MDM communication with SDC and SDS. Should be split up into two
# physical ports as bonding is really not recommended in production.
powerflex_cluster_interface=CHANGEME

# Virtual IP from which the cluster is available.
powerflex_cluster_vip=CHANGEME

# By default, All traffic including SDS-SDS and SDS-SDC goes to the same network
powerflex_rebuild_interface=CHANGEME

# Password used to access the PowerFlex OS cluster either by CLI or UI
powerflex_password=CHANGEME

# Password used by the Lightweight Integrated Agent when doing update or upgrade procedures.
powerflex_lia_token=CHANGEME

# nic_port_count = 5
# ProvisioningInterface=eno3
# Bond0Interface1=ens1f0
# Bond0Interface2=enp216s0f0
# Bond1Interface1=ens1f1
# Bond1Interface2=enp216s0f1
# private_api_vlanid=142
# private_api_network=192.168.142.0/24
# private_api_gateway=192.168.142.1
# private_api_allocation_pool_start=192.168.142.20
# private_api_allocation_pool_end=192.168.142.60
# tenant_network=192.168.132.0/24
# tenant_gateway=192.168.132.1
# tenant_vlanid=132
# tenant_allocation_pool_start=192.168.132.20
# tenant_allocation_pool_end=192.168.132.60
# storage_network=192.168.172.0/24
# storage_gateway=192.168.172.1
# storage_vlanid=172
# storage_allocation_pool_start=192.168.172.20
# storage_allocation_pool_end=192.168.172.60
# external_network=100.67.62.192/26
# external_vlanid=624
# external_gateway=100.67.62.193
# external_allocation_pool_start=100.67.62.200
# external_allocation_pool_end=100.67.62.210
# NFV site settings
# nfv_type must be one of dpdk, sriov or both
# nfv_type=sriov
# nfv_mtu=1500
# the number of NfvInterfaceN attributes is dependent on the number of Ports
# If ports are <= 7 you only need two NFVInterfaces > 7 implies four
# The order is important, pairs need to be across nics.
# nfv_interfaces=ens4f0,ens5f0,ens4f1,ens5f1
# HostNicDriver=vfio-pci
# BondInterfaceOvsOptions=bond_mode=balance-tcp lacp=active
# BondInterfaceSriovOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1
# NumDpdkInterfaceRxQueues = 1
# hpg_enable=true
# hpg_size=1GB
# numa_enable=true
# numa_hostos_cpu_count=4
# sriov_vf_count=4
