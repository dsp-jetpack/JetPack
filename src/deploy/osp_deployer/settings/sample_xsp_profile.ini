# Copyright (c) 2015-2019 Dell Inc. or its subsidiaries.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

###########################################################################
#                                                                         #
# Copy and rename this file for your own stamp.                           #
# Review ALL settings below, pay particular attention to paths/ip's etc.. #
#                                                                         #
###########################################################################


[Network Settings]

# Nova public network details
public_api_network=192.168.190.0/24
public_api_vlanid=190
public_api_gateway=192.168.190.1
public_api_netmask=255.255.255.0
public_api_allocation_pool_start=192.168.190.121
public_api_allocation_pool_end=192.168.190.250
name_server=8.8.8.8

# Private API network details
private_api_network=192.168.140.0/24
private_api_vlanid=140
private_api_netmask=255.255.255.0
private_api_allocation_pool_start=192.168.140.121
private_api_allocation_pool_end=192.168.140.250

# Storage network details
storage_network=192.168.170.0/24
storage_vlanid=170
storage_netmask=255.255.255.0
storage_allocation_pool_start=192.168.170.125
storage_allocation_pool_end=192.168.170.250

# Provisioning network details
provisioning_network=192.168.120.0/24
provisioning_vlanid=120
provisioning_netmask=255.255.255.0
provisioning_gateway=192.168.120.1
provisioning_net_dhcp_start=192.168.120.121
provisioning_net_dhcp_end=192.168.120.250
discovery_ip_range=192.168.120.21,192.168.120.120

# Storage cluster network details
storage_cluster_network=192.168.180.0/24
storage_cluster_vlanid=180
storage_cluster_allocation_pool_start=192.168.180.121
storage_cluster_allocation_pool_end=192.168.180.250

# Management network details
# Make sure the SAH node idrac ip defined in the .properties
# is NOT within the allocation pool below.
management_network=192.168.110.0/24
management_vlanid=110
management_netmask=255.255.255.0
management_gateway=192.168.110.1
management_allocation_pool_start=192.168.110.100
management_allocation_pool_end=192.168.110.199

# Tenant network details
# Not used unless you wish to configure Generic Routing Encapsulation (GRE) networks.
tenant_tunnel_network=192.168.130.0/24
tenant_tunnel_network_allocation_pool_start=192.168.130.121
tenant_tunnel_network_allocation_pool_end=192.168.130.250
tenant_tunnel_network_vlanid=130

# Nova Private network details
tenant_vlan_range=201:250


[MTU Settings]

# The mtu_selection setting defines whether to use the "global" or "per_network" option.
# If the mtu_selection is defined as "global", the mtu value for all networks will be set to the value provided in mtu_size_global_default. The supported value must be within the range of 1500-9000.
# If "per_network" mtu_selection is defined, the user should provide an mtu value for each network in the range of 1500-9000.
# For "public_api_network_mtu" and "floating_ip_network_mtu" networks, mtu sizes greater than 1500 is only supported if jumbo frames are enabled on upstream routers.
mtu_selection=global
mtu_size_global_default=1500
public_api_network_mtu=1500
floating_ip_network_mtu=1500
private_api_network_mtu=1500
tenant_network_mtu=1500
storage_cluster_network_mtu=1500
storage_network_mtu=1500
tenant_tunnel_network_mtu=1500

[Vips Settings]

# Use static VIPs ip addresses for the overcloud
use_static_vips=true

# The following VIP settings apply if the above use_static_vips is enabled.

# VIP for the redis service on the Private API api network
# Note that this IP must lie outside the private_api_allocation_pool_start/end
# range
redis_vip=192.168.140.251

# VIP for the provisioning network
# Note that this IP must lie outside the provisioning_net_dhcp_start/end range
provisioning_vip=192.168.120.251

# VIP for the Private API network
# Note that this IP must lie outside the private_api_allocation_pool_start/end
# range
private_api_vip=192.168.140.252

# VIP for the Public API network
# Note that this IP must lie outside the public_api_allocation_pool_start/end
# range
public_api_vip=192.168.190.251

# VIP for the Storage network
# Note that this IP must lie outside the storage_allocation_pool_start/end
# range
storage_vip=192.168.170.251

# VIP for the Storage cluster network
# The Storage Clustering network is not connected to the controller nodes,
# so the VIP for this network must be mapped to the provisioning network
# Note that this IP must lie outside the provisioning_net_dhcp_start/end range
storage_cluster_vip=192.168.120.252


[Subscription Manager Settings]

# Subscription Manager account info for registering Red Hat subscriptions
subscription_manager_user=xxxxxxx
subscription_manager_password=xxxxxxxxxxxxxx

# The following pool IDs provide different collections of repositories.
# Each is labeled with possible subscription names.

# Red Hat Enterprise Linux (Physical Node)
subscription_manager_pool_sah=xxxxxxxxxxxxxxxxxxxxxxxxxxxx44f5

# Red Hat Enterprise Linux OpenStack Platform (Virtual Node)
subscription_manager_pool_vm_rhel=xxxxxxxxxxxxxxxxxxxxxxxxxxxx454a

# Red Hat Ceph Storage (Physical Node)
subscription_manager_vm_ceph=xxxxxxxxxxxxxxxxxxxxxxxxxxxx7826

subscription_check_retries=20

# Red Hat Satellite Settings
use_satellite=false
satellite_ip=xxx.xxx.xxx.xx
satellite_hostname=samplehost.domain
satellite_org=sample_org
satellite_activation_key=sample_key

# Container images can be pulled from satellite rather than cdn if desired
pull_containers_from_satellite=false
# Satellite containers prefix, as per https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/13/html/director_installation_and_usage/configuring-a-container-image-source#Configuring-Registry_Details-Satellite
containers_prefix=organization-product-

[Nodes Nics and Bonding Settings]

sah_bond_opts=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1

# NIC environment file
#       1. To enable standard XSP profile or standard CSP profile (NUMA and HugePages):
#          For 5 ports, choose 5_port/nic_environment.yaml.
#          For 4 ports, choose 4_port/nic_environment.yaml.
#       2. To enable OVS-DPDK with 7 nic ports, choose ovs-dpdk_7_port/nic_environment.yaml.
#          For 9 ports, choose ovs-dpdk_9_port/nic_enviornment.yaml.
#       3. To enable SR-IOV with 7 nic ports, choose sriov_7_port/nic_environment.yaml.
#          For 9 ports, choose sriov_9_port/nic_enviornment.yaml.
#       4. To enable OVS-DPDK (2-ports) and SR-IOV (2-ports), choose ovs-dpdk_sriov_9_port/nic_environment.yaml.
#       5. To enable DVR, for Storage and Floating networks that share a single bond, choose 5_port/nic_environment.yaml.
#          If a separate bond is required for Floating network on compute nodes, choose dvr_7_port/nic_environment.yaml.
nic_env_file=5_port/nic_environment.yaml

# In case of ovs-dpdk_7_port, ovs-dpdk_9_port or ovs-dpdk_sriov_9_port configuration
# Choose the appropriate driver for the NICs being used. For Intel, use "vfio-pci"
# and for Mellanox use "mlx5_core"
# The Following line should be commented out if ovs_dpdk_enable is set to false
#HostNicDriver=mlx5_core

# Interfaces and bonding options per node type.
# When using any 4 port NIC configuration, comment out or delete the
# ControllerProvisioningInterface line below.
ControllerProvisioningInterface=em3
ControllerBond0Interface1=em1
ControllerBond0Interface2=p1p1
ControllerBond1Interface1=em2
ControllerBond1Interface2=p1p2
ControllerBondInterfaceOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1

# When using any 4 port NIC configuration, comment out or delete the
# ComputeProvisioningInterface line below.
ComputeProvisioningInterface=em3
ComputeBond0Interface1=em1
ComputeBond0Interface2=p1p1
ComputeBond1Interface1=em2
ComputeBond1Interface2=p1p2
ComputeBondInterfaceOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1

# When using any 4 port NIC configuration, comment out or delete the
# StorageProvisioningInterface line below.
StorageProvisioningInterface=em3
StorageBond0Interface1=em1
StorageBond0Interface2=p2p1
StorageBond1Interface1=em2
StorageBond1Interface2=p2p2
StorageBondInterfaceOptions=mode=802.3ad miimon=100 xmit_hash_policy=layer3+4 lacp_rate=1

# To enable standalone OVS-DPDK, two or four interfaces should be used.
# For two interfaces, uncomment 'ComputeOvsDpdkInterface1', 'ComputeOvsDpdkInterface2' and 'BondInterfaceOvsOption'.
# For four interfaces, uncomment all four interfaces and 'BondInterfaceOvsOption'.
# The following lines should be commented out  if ovs_dpdk_enable is set to false
#ComputeOvsDpdkInterface1=p2p1
#ComputeOvsDpdkInterface2=p3p1
#ComputeOvsDpdkInterface3=p2p2
#ComputeOvsDpdkInterface4=p3p2
#BondInterfaceOvsOptions=bond_mode=balance-tcp lacp=active

# To enable standalone SR-IOV, two or four interfaces should be used.
# For two interfaces, uncomment 'ComputeSriovInterface1' and 'ComputeSriovInterface2'.
# For four interfaces, uncomment all four interfaces.
# Following lines should be commented out if sriov_enable is set to false
#ComputeSriovInterface1=p1p1
#ComputeSriovInterface2=p4p1
#ComputeSriovInterface3=p1p2
#ComputeSriovInterface4=p4p2

# To enable SR-IOV and OVS-DPDK, four interfaces should be used.
# Following lines should be uncommented if both sriov_enable and ovs-dpdk_enable are set to true.
#ComputeSriovInterface1=p1p1
#ComputeSriovInterface2=p4p1
#ComputeOvsDpdkInterface1=p2p1
#ComputeOvsDpdkInterface2=p3p1
#BondInterfaceOvsOptions=bond_mode=balance-tcp lacp=active

# To enable DVR with a separate bond for Floating network on compute nodes, two interfaces should be used.
# To use the two interfaces, uncomment 'ComputeBond2Interface1' and 'ComputeBond2Interface2'.
# Following lines should be commented out if dvr_enable is set to false.
#ComputeBond2Interface1: p3p1
#ComputeBond2Interface2: p1p2

[Dell NFV Settings]

# Enter value of enable_hpg as true/false for HugePages
hpg_enable=false
# User should give this parameter in same format.
# Supported values for hpg_size(Size of hugepages) is 2MB and 1 GB.
# The number of hugepages will be calculated dynamically.
hpg_size=1GB

# Enter value of enable_numa as true/false for NUMA
numa_enable=false

# OVS-DPDK Settings
# Set the following option to true/false
ovs_dpdk_enable=false

# Enter number of cores you want to reserve for Host OS
# Supported values are 2,4,6,8
numa_hostos_cpu_count=4

# SRIOV Settings
# Set the following option to true/false for SRIOV
sriov_enable=false

# Set the following option to true/false for Smart NICs
# Smart NIC option should be set to true to enable SRIOV hardware offload
smart_nic=false 

# Enter the number of VFs you want to create per port
# Supported values are between 1-64
sriov_vf_count=64

# Set the following option to true/false for DVR
dvr_enable=false

# Set the following option to true/false for Barbican
barbican_enable=false

# Set the following option to true/false for Octavia
octavia_enable=false
# Set the octavia_generate_certs parameter to false to
# provide your own certificates and keys to octavia
# If octavia_generate_certs is set to false, provide a custom
# environment file containing your certificates and keys in yaml format
octavia_generate_certs=true
certificate_keys_path=/pathto/octavia-environment.yaml

[Performance and Optimization]

# mariadb_max_connections takes value from 1000 to 100000, it is mandatory.
mariadb_max_connections = 15360

# MariaDB innodb_buffer_pool_size should be given value in GB, Example : 64G.
# Default is 'dynamic' which assigns 75% ram size of controller node.
# Note that innodb_buffer_pool_size should be less than available ram size.
innodb_buffer_pool_size = dynamic

# innodb_buffer_pool_instances takes value from 8 to 48
innodb_buffer_pool_instances = 16

[IPMI credentials Settings]

# DRAC credentials with IPMI privilege for the SAH node
sah_ipmi_user=root
sah_ipmi_password=xxxxxxx

# DRAC credentials with IPMI privilege for the overcloud nodes
ipmi_user=root
ipmi_password=xxxxxxx
# A password to change to on overcloud nodes if desired
new_ipmi_password=


[Deployment Settings]

# Valid values are csp, xsp and custom.
profile=xsp

# This pathname must be the full path to the properties file which
# describes the cluster. You should copy *this* sample settings file
# (sample.ini) and the sample properties file (sample.properties) to
# another directory, and customize them for your cluster. Then use the
# path to your customized properties file here.
cluster_nodes_configuration_file=/root/acme.properties

# User for the undercloud/overcloud installation
director_install_user=osp_admin
director_install_user_password=xxxxxxx

# Name of the Overcloud.
# The nodes hostnames will be prepended with the given name and a dash
overcloud_name=overcloud

# Domain name for the cluster (i.e., mycluster.lab)
domain=domain.net

# Optional : root passord for the overcloud nodes, no password is set if left empty
overcloud_nodes_pwd=

# , separated list of ntp servers
ntp_servers=0.centos.pool.ntp.org,1.centos.pool.ntp.org,2.centos.pool.ntp.org,3.centos.pool.ntp.org
time_zone=America/Chicago

# Use static ip adresses for the overcloud nodes if set to true (ips need to be defined in the .properties)
# Use dhcp if set to false (ips not required in the .properties)
overcloud_static_ips=true

# Set to true to enable cinder backend of Ceph for storage.
enable_rbd_backend=true

# Set to true to enable Nova usage of Ceph for ephemeral storage.
# If set to false, Nova uses the storage local to the compute.
enable_rbd_nova_backend=true

# Set the glance backend. Values are file, rbd, swift or cinder
glance_backend=rbd

# Set to false to disable fencing
enable_fencing=true


[Storage back-end Settings]

# Storage Backends: Ceph, Dell EMC Unity, Dell EMC Storage Center
#       1. To enable Ceph as the storage backend in the solution:
#            For Cinder/Block Storage:
#              set 'enable_rbd_backend=true'
#              set 'enable_dellsc_backend=false'
#              set 'enable_unity_backend=false'
#            For Manila/Share Storage(Not Supported Yet):
#              set 'enable_unity_manila_backend=false'
#            For Glance/Image Storage:
#              set 'glance_backend=rbd'
#            For Nova Ephemeral Storage,
#              set 'enable_rbd_nova_backend=true'
#       2. To enable Dell EMC Unity as the storage backend in the solution:
#            For Cinder/Block Storage:
#              set 'enable_rbd_backend=false'
#              set 'enable_dellsc_backend=false'
#              set 'enable_unity_backend=true'
#            For Manila/Share Storage:
#              set 'enable_unity_manila_backend=true'
#            For Glance/Image Service:
#              set 'glance_backend=cinder'
#            For Nova Ephemeral Storage, false defaults to local
#              set 'enable_rbd_nova_backend=false'
#       3. To enable Dell EMC Storage Center as the storage backend in the solution:
#            For Cinder/Block Storage:
#              set 'enable_rbd_backend=false'
#              set 'enable_dellsc_backend=true'
#              set 'enable_unity_backend=false'
#            For Manila/Share Storage(Not Supported Yet):
#              set 'enable_unity_manila_backend=false'
#            For Glance/Image Service:
#              set 'glance_backend=cinder'
#            For Nova Ephemeral Storage, false defaults to local
#              set 'enable_rbd_nova_backend=false'
#       4. For a multi-backend deployment, with Ceph and Dell EMC Unity as backends:
#            For Cinder/Block Storage:
#              set 'enable_rbd_backend=true'
#              set 'enable_dellsc_backend=false'
#              set 'enable_unity_backend=true'
#            For Manila/Share Storage:
#              set 'enable_unity_manila_backend=true'
#            For Glance/Image Service, rbd or cinder:
#              set 'glance_backend=rbd'
#            For Nova Ephemeral Storage
#              set 'enable_rbd_nova_backend=true'


# Dell Storage Center cinder parameters.
enable_dellsc_backend=false
dellsc_backend_name=CHANGEME
dellsc_api_port=3033
dellsc_iscsi_ip_address=CHANGEME
dellsc_iscsi_port=3260
dellsc_san_ip=CHANGEME
dellsc_san_login=CHANGEME
dellsc_san_password=CHANGEME
dellsc_ssn=CHANGEME
dellsc_server_folder=cmpl_iscsi_servers
dellsc_volume_folder=cmpl_iscsi_volumes

#
# Unity cinder parameters.
#
enable_unity_backend=false
# The Dell EMC Unity Container is published in the RH Container Catalog
# registry.connect.redhat.com
# By default, the latest image of the RHOSP release is pulled.
# Change to a different version if a specific image is required.
cinder_unity_container_version=latest
unity_backend_name=CHANGEME
unity_san_ip=CHANGEME
unity_san_login=CHANGEME
unity_san_password=CHANGEME
unity_storage_protocol=iSCSI
unity_io_ports=CHANGEME
unity_storage_pool_names=CHANGEME

#
# Unity manila parameters.
#
enable_unity_manila_backend=false
# The Dell EMC Unity Container is published in the RH Container Catalog
# registry.connect.redhat.com
# By default, the latest image of the RHOSP release is pulled.
# Change to a different version if a specific image is required.
manila_unity_container_version=latest
manila_unity_driver_handles_share_servers=true
manila_unity_nas_login=CHANGEME
manila_unity_nas_password=CHANGEME
manila_unity_nas_server=CHANGEME
manila_unity_server_meta_pool=CHANGEME
manila_unity_share_data_pools=CHANGEME
manila_unity_ethernet_ports=CHANGEME
manila_unity_ssl_cert_verify=false
manila_unity_ssl_cert_path=''

[Sanity Test Settings]

# If you want the sanity script to run on deployment completion (Appendix C, etc.), you may do so.
run_sanity=false

floating_ip_network=192.168.191.0/24
floating_ip_network_start_ip=192.168.191.20
floating_ip_network_end_ip=192.168.191.59
floating_ip_network_gateway=192.168.191.1
floating_ip_network_vlan=191
sanity_tenant_network=192.168.201.0/24
sanity_user_password=s@n1ty
sanity_user_email=someone@somewhere.com
sanity_key_name=sanity

# The number of instances to spin up in nova.
# Note that this will be limited by the instance quota in OpenStack, which is
# 10 by default.
sanity_number_instances=1
sanity_image_url=http://cloud.centos.org/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2

# vlan-aware specific parameters
# Set the following option to true/false for vlan aware sanity test. If set to true, one additional
# instance will be created for vlan-aware testing.
vlan_aware_sanity=false
# address of vlan-network where subport is attached
sanity_vlantest_network=192.168.216.0/24

[Tempest Settings]

# If you want to run Tempest post-deployment, you may do so.
# Note: The sanity test script must be run first to create networks for Tempest.
run_tempest=false

# The flag tempest_smoke_only is dependent on run_tempest=true for
# tempest_smoke_only to have any effect.
# If run_tempest=true and tempest_smoke_only=true, only a subset of tempest tests
# tagged as smoke tests will be executed.  This set of tests is useful for validating
# basic OpenStack functionality.
tempest_smoke_only=true

# The tempest workspace directory which will be created in the user's home
# directory, and initialized when tempest is configured.
tempest_workspace=mytempest

[Advanced Settings]

#The following settings should typically only be used by developers

# Only developers should set to false.
enable_version_locking=true

# The list of RHSM repositories to enable to access the product.  Repos should
# be comma separated.
# Note that this parameter is defaulted as shown below when commented out or
# not specified.  It should not be necessary to change it from the default in
# most cases.
rhsm_repos=rhel-7-server-openstack-13-rpms,rhel-7-server-openstack-13-devtools-rpms,rhel-7-server-rhceph-3-tools-rpms

# Option below is to use a custom instack.json and skip discover_nodes
use_custom_instack_json=false
custom_instack_json=n/a

# Indicates if the deploy-overcloud.py script should be run in debug mode
deploy_overcloud_debug=false

use_internal_repo=false
# Semi-colon ( ; ) separated list of internal repos to use, if needed.
internal_repos_locations=CHANGEME_INTERNAL_REPO_URL

cloud_repo_dir=/root/JetPack
rhel_iso=/root/rhel76.iso

# Overcloud deployment timeout value - default is 180mns, but can be tweaked here if required.
overcloud_deploy_timeout=180

# Default driver is DRAC.
use_ipmi_driver=false

# Default introspection method is out-of-band.
# Note that out-of-band introspection is only supported by the DRAC driver.  If
# use_ipmi_driver is set to "true" above then in-band introspection will be
# used regardless of the value below.
use_in_band_introspection=false

# RDO cloud images
# Available to download @ https://access.redhat.com/downloads/content/191/ver=8/rhel---7/8/x86_64/product-software
discovery_ram_disk_image=/pathto/discovery-ramdisk-7.1.0-39.tar
overcloud_image=/pathto/overcloud-full-7.1.0-39.tar
# if option below is enabled, images will be pulled fom the cdn (and the above x2 settings ignored)
pull_images_from_cdn=true

# Occasionally there can be problems with Subscription Manager
# and a node may be properly registered yet "subscription manager status"
# will return "Unknown"  which will cause checks to fail.
# Setting this to false will skip SM checks to get around this issue.
verify_rhsm_status=true
