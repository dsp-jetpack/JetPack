# Copyright (c) 2016 Dell Inc. or its subsidiaries.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

resource_registry:
    OS::TripleO::NodeUserData: ./first-boot.yaml
    OS::TripleO::NodeExtraConfigPost: ./post-deploy.yaml
    # These resources disable Swift
    OS::TripleO::Services::SwiftProxy: OS::Heat::None
    OS::TripleO::Services::SwiftStorage: OS::Heat::None
    OS::TripleO::Services::SwiftRingBuilder: OS::Heat::None

parameter_defaults:
  # To customize the domain name of the overcloud nodes, change "localdomain"
  # in the following line to the desired domain name.
  CloudDomain: localdomain

  # Set to true to enable Nova usage of Ceph for ephemeral storage.
  # If set to false, Nova uses the storage local to the compute.
  NovaEnableRbdBackend: true

  # Configure Ceph Placement Group (PG) values for the indicated pools
  CephPools:
    volumes:
      pg_num: 1024
      pgp_num: 1024
    backups:
      pg_num: 512
      pgp_num: 512
    vms:
      pg_num: 256
      pgp_num: 256
    images:
      pg_num: 128
      pgp_num: 128
    .rgw.buckets:
      pg_num: 512
      pgp_num: 512

  # Additional hiera data for all nodes
  ExtraConfig:
    ceph::profile::params::osd_journal_size: 10000
    ceph::profile::params::osd_pool_default_min_size: 2
    # CHANGEME:
    # Modify the 'osds' parameter to reflect the list of drives to be used as
    # OSDs. A configuration that colocates Ceph journals on every OSD should
    # look like this:
    # ceph::profile::params::osds:
    #   '/dev/sdb': {}
    #   '/dev/sdc': {}
    #   ... and so on.
    # A configuration that places Ceph journals on dedicated drives (such as
    # SSDs) should look like this:
    # ceph::profile::params::osds:
    #   '/dev/sde':
    #     journal: '/dev/sdb'
    #   '/dev/sdf':
    #     journal: '/dev/sdb'
    #   ... and so on.
    ceph::profile::params::osds:
      '/dev/sdd':
        journal: '/dev/sda'
      '/dev/sde':
        journal: '/dev/sda'
      '/dev/sdf':
        journal: '/dev/sda'
      '/dev/sdg':
        journal: '/dev/sda'
      '/dev/sdh':
        journal: '/dev/sdb'
      '/dev/sdi':
        journal: '/dev/sdb'
      '/dev/sdj':
        journal: '/dev/sdb'
      '/dev/sdk':
        journal: '/dev/sdb'
      '/dev/sdl':
        journal: '/dev/sdc'
      '/dev/sdm':
        journal: '/dev/sdc'
      '/dev/sdn':
        journal: '/dev/sdc'
      '/dev/sdo':
        journal: '/dev/sdc'

    # Additional entries added to /etc/ceph/ceph.conf
    ceph::conf::args:
      global/max_open_files:
        value: 131072
      client.radosgw.gateway/rgw_keystone_url:
        value: "%{hiera('swift::proxy::authtoken::identity_uri')}"
      client.radosgw.gateway/rgw_keystone_admin_token:
        value: "%{hiera('keystone::admin_token')}"
      client.radosgw.gateway/rgw_keystone_accepted_roles:
        value: "_member_, Member, admin, swift"
      client.radosgw.gateway/rgw_keystone_token_cache size:
        value: 500
      client.radosgw.gateway/rgw_keystone_revocation_interval:
        value: 600
      client.radosgw.gateway/rgw_keystone_make_new_tenants:
        value: true
      client.radosgw.gateway/rgw_s3_auth_use_keystone:
        value: true
      client.radosgw.gateway/rgw_init_timeout:
        value: 1200

  # Additional hiera data for storage nodes
  CephStorageExtraConfig:
    ceph_osd_selinux_permissive: false

  # Additional hiera data for controller nodes
  controllerExtraConfig:
    ceph::profile::params::enable_rgw: true
    ceph::profile::params::rgw_frontends: "civetweb port=7480"
