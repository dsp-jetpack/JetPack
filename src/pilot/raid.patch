--- raid_original.py	2019-12-30 10:29:42.226859376 +0000
+++ raid_modified.py	2019-12-30 10:29:18.381080327 +0000
@@ -15,6 +15,7 @@
 DRAC RAID specific methods
 """
 
+from collections import defaultdict
 import math
 
 from futurist import periodics
@@ -34,6 +35,7 @@
 from ironic.drivers.modules.drac import common as drac_common
 from ironic.drivers.modules.drac import job as drac_job
 
+drac_constants = importutils.try_import('dracclient.constants')
 drac_exceptions = importutils.try_import('dracclient.exceptions')
 
 LOG = logging.getLogger(__name__)
@@ -134,6 +136,34 @@
         raise exception.DracOperationError(error=exc)
 
 
+def _is_raid_controller(node, raid_controller_fqdd, raid_controllers=None):
+    """Find out if object's fqdd is for a raid controller or not
+
+    :param node: an ironic node object
+    :param raid_controller_fqdd: The object's fqdd we are testing to see
+                                 if it is a raid controller or not.
+    :param raid_controllers: A list of RAIDControllers used to check for
+                             the presence of BOSS cards.  If None, the
+                             iDRAC will be queried for the list of
+                             controllers.
+    :returns: boolean, True if the device is a RAID controller,
+              False if not.
+    """
+    client = drac_common.get_drac_client(node)
+
+    try:
+        return client.is_raid_controller(raid_controller_fqdd,
+                                         raid_controllers)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('Unable to determine if controller %(raid_controller_fqdd)s '
+                  'on node %(node_uuid)s is a RAID controller. '
+                  'Reason: %(error)s. ',
+                  {'raid_controller_fqdd': raid_controller_fqdd,
+                   'node_uuid': node.uuid, 'error': exc})
+
+        raise exception.DracOperationError(error=exc)
+
+
 def create_virtual_disk(node, raid_controller, physical_disks, raid_level,
                         size_mb, disk_name=None, span_length=None,
                         span_depth=None):
@@ -202,7 +232,108 @@
         raise exception.DracOperationError(error=exc)
 
 
-def commit_config(node, raid_controller, reboot=False):
+def _reset_raid_config(node, raid_controller):
+    """Delete all virtual disk and unassign all hotspares physical disk
+
+    :param node: an ironic node object.
+    :param raid_controller: id of the RAID controller.
+    :returns: a dictionary containing
+              - The is_commit_required needed key with a
+              boolean value indicating whether a config job must be created
+              for the values to be applied.
+              - The is_reboot_required key with a RebootRequired enumerated
+              value indicating whether the server must be rebooted to
+              reset configuration.
+    :raises: DracOperationError on an error from python-dracclient.
+    """
+    try:
+
+        drac_job.validate_job_queue(node)
+
+        client = drac_common.get_drac_client(node)
+        return client.reset_raid_config(raid_controller)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('DRAC driver failed to delete all virtual disk '
+                  'and unassign all hotspares '
+                  'on %(raid_controller_fqdd)s '
+                  'for node %(node_uuid)s. '
+                  'Reason: %(error)s.',
+                  {'raid_controller_fqdd': raid_controller,
+                   'node_uuid': node.uuid,
+                   'error': exc})
+        raise exception.DracOperationError(error=exc)
+
+
+def clear_foreign_config(node, raid_controller):
+    """Free up the foreign drives.
+
+    :param node: an ironic node object.
+    :param raid_controller: id of the RAID controller.
+    :returns: a dictionary containing
+              - The is_commit_required needed key with a
+              boolean value indicating whether a config job must be created
+              for the values to be applied.
+              - The is_reboot_required key with a RebootRequired enumerated
+              value indicating whether the server must be rebooted to
+              clear foreign configuration.
+    :raises: DracOperationError on an error from python-dracclient.
+    """
+    try:
+
+        drac_job.validate_job_queue(node)
+
+        client = drac_common.get_drac_client(node)
+        return client.clear_foreign_config(raid_controller)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('DRAC driver failed to free foreign driver '
+                  'on %(raid_controller_fqdd)s '
+                  'for node %(node_uuid)s. '
+                  'Reason: %(error)s.',
+                  {'raid_controller_fqdd': raid_controller,
+                   'node_uuid': node.uuid,
+                   'error': exc})
+        raise exception.DracOperationError(error=exc)
+
+
+
+def change_physical_disk_state(node, mode=None,
+                               controllers_to_physical_disk_ids=None):
+    """Convert disks RAID status
+
+    This method converts the requested physical disks from
+    RAID to JBOD or vice versa.  It does this by only converting the
+    disks that are not already in the correct state.
+
+    :param node: an ironic node object.
+    :param mode: the mode to change the disks either to RAID or JBOD.
+    :param controllers_to_physical_disk_ids: Dictionary of controllers and
+           corresponding disk ids to convert to the requested mode.
+    :return: a dictionary containing:
+             - conversion_results, a dictionary that maps controller ids
+             to the conversion results for that controller.
+             The conversion results are a dict that contains:
+             - The is_commit_required key with the value always set to
+             True indicating that a config job must be created to
+             complete disk conversion.
+             - The is_reboot_required key with a RebootRequired
+             enumerated value indicating whether the server must be
+             rebooted to complete disk conversion.
+    :raises: DRACOperationError on an error from python-dracclient.
+    """
+    try:
+        drac_job.validate_job_queue(node)
+        client = drac_common.get_drac_client(node)
+        return client.change_physical_disk_state(
+            mode, controllers_to_physical_disk_ids)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('DRAC driver failed to change physical drives '
+                  'to %(mode)s mode for node %(node_uuid)s. '
+                  'Reason: %(error)s.',
+                  {'mode': mode, 'node_uuid': node.uuid, 'error': exc})
+        raise exception.DracOperationError(error=exc)
+
+
+def commit_config(node, raid_controller, reboot=False, realtime=False):
     """Apply all pending changes on a RAID controller.
 
     :param node: an ironic node object.
@@ -215,7 +346,9 @@
     client = drac_common.get_drac_client(node)
 
     try:
-        return client.commit_pending_raid_changes(raid_controller, reboot)
+        return client.commit_pending_raid_changes(raid_controller=raid_controller,
+                                                  reboot=reboot,
+                                                  realtime=realtime)
     except drac_exceptions.BaseClientException as exc:
         LOG.error('DRAC driver failed to commit pending RAID config for'
                   ' controller %(raid_controller_fqdd)s on node '
@@ -226,6 +359,33 @@
         raise exception.DracOperationError(error=exc)
 
 
+def _change_physical_disk_mode(node, mode=None,
+                               controllers_to_physical_disk_ids=None):
+    """Physical drives conversion from RAID to JBOD or vice-versa.
+
+    :param node: an ironic node object.
+    :param mode: the mode to change the disks either to RAID or JBOD.
+    :param controllers_to_physical_disk_ids: Dictionary of controllers and
+           corresponding disk ids to convert to the requested mode.
+    :returns: states.CLEANWAIT if deletion is in progress asynchronously
+              or None if it is completed.
+    """
+    change_disk_state = change_physical_disk_state(
+        node, mode, controllers_to_physical_disk_ids)
+
+    controllers = list()
+    conversion_results = change_disk_state['conversion_results']
+    for controller_id, result in conversion_results.items():
+        controller = {'raid_controller': controller_id,
+                      'is_reboot_required': result['is_reboot_required'],
+                      'is_commit_required': result['is_commit_required']}
+        controllers.append(controller)
+
+    return _commit_to_controllers(
+        node,
+        controllers, substep='completed')
+
+
 def abandon_config(node, raid_controller):
     """Deletes all pending changes on a RAID controller.
 
@@ -630,35 +790,98 @@
     return filtered_disks
 
 
-def _commit_to_controllers(node, controllers):
-    """Commit changes to RAID controllers on the node."""
+
+def _create_config_job(node, controller, reboot=False, realtime=False,
+                       raid_config_job_ids=[],
+                       raid_config_parameters=[]):
+    job_id = commit_config(node, raid_controller=controller,
+                           reboot=reboot, realtime=realtime)
+
+    raid_config_job_ids.append(job_id)
+    if controller not in raid_config_parameters:
+        raid_config_parameters.append(controller)
+
+    LOG.info('Change has been committed to RAID controller '
+             '%(controller)s on node %(node)s. '
+             'DRAC job id: %(job_id)s',
+             {'controller': controller, 'node': node.uuid,
+              'job_id': job_id})
+    return {'raid_config_job_ids': raid_config_job_ids,
+            'raid_config_parameters': raid_config_parameters}
+
+
+def _commit_to_controllers(node, controllers, substep="completed"):
+    """Commit changes to RAID controllers on the node.
+
+    :param node: an ironic node object
+    :param controllers: a list of dictionary containing
+                        - The raid_controller key with raid controller
+                        fqdd value indicating on which raid configuration
+                        job needs to be perform.
+                        - The is_commit_required needed key with a
+                        boolean value indicating whether a config job must
+                        be created.
+                        - The is_reboot_required key with a RebootRequired
+                        enumerated value indicating whether the server must
+                        be rebooted only if raid controller does not support
+                        realtime.
+    :param substep: contain sub cleaning step which executes any raid
+                    configuration job if set after cleaning step.
+                    (default to completed)
+    :returns: states.CLEANWAIT if deletion is in progress asynchronously
+              or None if it is completed.
+    """
+    # remove controller which does not require configuration job
+    controllers = [controller for controller in controllers
+                   if controller['is_commit_required']]
 
     if not controllers:
         LOG.debug('No changes on any of the controllers on node %s',
                   node.uuid)
+        driver_internal_info = node.driver_internal_info
+        driver_internal_info['raid_config_substep'] = substep
+        driver_internal_info['raid_config_parameters'] = []
+        node.driver_internal_info = driver_internal_info
+        node.save()
         return
 
     driver_internal_info = node.driver_internal_info
+    driver_internal_info['raid_config_substep'] = substep
+    driver_internal_info['raid_config_parameters'] = []
+
     if 'raid_config_job_ids' not in driver_internal_info:
         driver_internal_info['raid_config_job_ids'] = []
 
-    controllers = list(controllers)
-    for controller in controllers:
-        # Do a reboot only for the last controller
-        if controller == controllers[-1]:
-            job_id = commit_config(node, raid_controller=controller,
-                                   reboot=True)
-        else:
-            job_id = commit_config(node, raid_controller=controller,
-                                   reboot=False)
+    all_realtime = True
+    optional = drac_constants.RebootRequired.optional
+    all_realtime = all(cntlr['is_reboot_required'] == optional
+                       for cntlr in controllers)
+    raid_config_job_ids = []
+    raid_config_parameters = []
+    if all_realtime:
+        for controller in controllers:
+            realtime_controller = controller['raid_controller']
+            job_details = _create_config_job(
+                node, controller=realtime_controller,
+                reboot=False, realtime=True,
+                raid_config_job_ids=raid_config_job_ids,
+                raid_config_parameters=raid_config_parameters)
+
+    else:
+        for controller in controllers:
+            mix_controller = controller['raid_controller']
+            reboot = (controller == controllers[-1])
+            job_details = _create_config_job(
+                node, controller=mix_controller,
+                reboot=reboot, realtime=False,
+                raid_config_job_ids=raid_config_job_ids,
+                raid_config_parameters=raid_config_parameters)
 
-        LOG.info('Change has been committed to RAID controller '
-                 '%(controller)s on node %(node)s. '
-                 'DRAC job id: %(job_id)s',
-                 {'controller': controller, 'node': node.uuid,
-                  'job_id': job_id})
+    driver_internal_info['raid_config_job_ids'].extend(job_details[
+        'raid_config_job_ids'])
 
-        driver_internal_info['raid_config_job_ids'].append(job_id)
+    driver_internal_info['raid_config_parameters'].extend(job_details[
+        'raid_config_parameters'])
 
     node.driver_internal_info = driver_internal_info
     node.save()
@@ -735,10 +958,10 @@
         logical_disks_to_create = _filter_logical_disks(
             logical_disks, create_root_volume, create_nonroot_volumes)
 
-        controllers = set()
+        controllers = list()
         for logical_disk in logical_disks_to_create:
-            controllers.add(logical_disk['controller'])
-            create_virtual_disk(
+            controller = dict()
+            controller_cap = create_virtual_disk(
                 node,
                 raid_controller=logical_disk['controller'],
                 physical_disks=logical_disk['physical_disks'],
@@ -747,8 +970,15 @@
                 disk_name=logical_disk.get('name'),
                 span_length=logical_disk.get('span_length'),
                 span_depth=logical_disk.get('span_depth'))
+            controller['raid_controller'] = logical_disk['controller']
+            controller['is_reboot_required'] = controller_cap[
+                'is_reboot_required']
+            controller['is_commit_required'] = controller_cap[
+                'is_commit_required']
+            if controller not in controllers:
+                controllers.append(controller)
 
-        return _commit_to_controllers(node, list(controllers))
+        return _commit_to_controllers(node, controllers)
 
     @METRICS.timer('DracRAID.delete_configuration')
     @base.clean_step(priority=0)
@@ -762,12 +992,21 @@
         """
         node = task.node
 
-        controllers = set()
-        for disk in list_virtual_disks(node):
-            controllers.add(disk.controller)
-            delete_virtual_disk(node, disk.id)
+        controllers = list()
+        drac_raid_controllers = list_raid_controllers(node)
+        for cntrl in drac_raid_controllers:
+            if _is_raid_controller(node, cntrl.id, drac_raid_controllers):
+                controller = dict()
+                controller_cap = _reset_raid_config(node, cntrl.id)
+                controller["raid_controller"] = cntrl.id
+                controller["is_reboot_required"] = controller_cap[
+                    "is_reboot_required"]
+                controller["is_commit_required"] = controller_cap[
+                    "is_commit_required"]
+                controllers.append(controller)
 
-        return _commit_to_controllers(node, list(controllers))
+        return _commit_to_controllers(node, controllers,
+                                      substep="delete_foreign_config")
 
     @METRICS.timer('DracRAID.get_logical_disks')
     def get_logical_disks(self, task):
@@ -840,9 +1079,9 @@
         for config_job_id in raid_config_job_ids:
             config_job = drac_job.get_job(node, job_id=config_job_id)
 
-            if config_job.state == 'Completed':
+            if config_job is None or config_job.status == 'Completed':
                 finished_job_ids.append(config_job_id)
-            elif config_job.state == 'Failed':
+            elif config_job.status == 'Failed':
                 finished_job_ids.append(config_job_id)
                 self._set_raid_config_job_failure(node)
 
@@ -852,13 +1091,60 @@
         task.upgrade_lock()
         self._delete_cached_config_job_id(node, finished_job_ids)
 
-        if not node.driver_internal_info['raid_config_job_ids']:
-            if not node.driver_internal_info.get('raid_config_job_failure',
-                                                 False):
-                self._resume_cleaning(task)
+
+        if not node.driver_internal_info.get('raid_config_job_failure',
+                                             False):
+            if 'raid_config_substep' in node.driver_internal_info:
+                if node.driver_internal_info['raid_config_substep'] == \
+                        'delete_foreign_config':
+                    self._execute_cleaning_foreign_drives(task, node)
+                elif node.driver_internal_info['raid_config_substep'] == \
+                        'completed':
+                    self._complete_raid_cleaning_substep(task, node)
             else:
-                self._clear_raid_config_job_failure(node)
-                self._set_clean_failed(task, config_job)
+                self._complete_raid_cleaning_substep(task, node)
+        else:
+            self._clear_raid_substep(node)
+            self._clear_raid_config_job_failure(node)
+            self._set_clean_failed(task, config_job)
+
+    def _execute_cleaning_foreign_drives(self, task, node):
+        controllers = list()
+        jobs_required = False
+        for controller_id in node.driver_internal_info[
+                'raid_config_parameters']:
+            controller_cap = clear_foreign_config(
+                node, controller_id)
+            controller = {
+                'raid_controller': controller_id,
+                'is_reboot_required': controller_cap['is_reboot_required'],
+                'is_commit_required': controller_cap['is_commit_required']}
+
+            controllers.append(controller)
+            jobs_required = jobs_required or controller_cap[
+                'is_commit_required']
+
+        if not jobs_required:
+            LOG.info(
+                "No foreign drives detected, so "
+                "resume cleaning")
+            self._complete_raid_cleaning_substep(task, node)
+        else:
+            _commit_to_controllers(
+                node,
+                controllers,
+                substep='completed')
+
+    def _complete_raid_cleaning_substep(self, task, node):
+        self._clear_raid_substep(node)
+        self._resume_cleaning(task)
+
+    def _clear_raid_substep(self, node):
+        driver_internal_info = node.driver_internal_info
+        driver_internal_info.pop('raid_config_substep', None)
+        driver_internal_info.pop('raid_config_parameters', None)
+        node.driver_internal_info = driver_internal_info
+        node.save()
 
     def _set_raid_config_job_failure(self, node):
         driver_internal_info = node.driver_internal_info
