--- raid.py.orig	2019-10-08 04:56:55.974951569 -0400
+++ raid.py	2019-10-08 05:22:51.224425273 -0400
@@ -34,6 +34,7 @@
 from ironic.drivers.modules.drac import common as drac_common
 from ironic.drivers.modules.drac import job as drac_job
 
+drac_constants = importutils.try_import('dracclient.constants')
 drac_exceptions = importutils.try_import('dracclient.exceptions')
 
 LOG = logging.getLogger(__name__)
@@ -134,6 +135,34 @@
         raise exception.DracOperationError(error=exc)
 
 
+def _is_raid_controller(node, raid_controller_fqdd, raid_controllers=None):
+    """Find out if object's fqdd is for a raid controller or not
+
+    :param node: an ironic node object
+    :param raid_controller_fqdd: The object's fqdd we are testing to see
+                                 if it is a raid controller or not.
+    :param raid_controllers: A list of RAIDControllers used to check for
+                             the presence of BOSS cards.  If None, the
+                             iDRAC will be queried for the list of
+                             controllers.
+    :returns: boolean, True if the device is a RAID controller,
+              False if not.
+    """
+    client = drac_common.get_drac_client(node)
+
+    try:
+        return client.is_raid_controller(raid_controller_fqdd,
+                                         raid_controllers)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('Unable to determine if controller %(raid_controller_fqdd)s '
+                  'on node %(node_uuid)s is a RAID controller. '
+                  'Reason: %(error)s. ',
+                  {'raid_controller_fqdd': raid_controller_fqdd,
+                   'node_uuid': node.uuid, 'error': exc})
+
+        raise exception.DracOperationError(error=exc)
+
+
 def create_virtual_disk(node, raid_controller, physical_disks, raid_level,
                         size_mb, disk_name=None, span_length=None,
                         span_depth=None):
@@ -202,7 +231,70 @@
         raise exception.DracOperationError(error=exc)
 
 
-def commit_config(node, raid_controller, reboot=False):
+def _reset_raid_config(node, raid_controller):
+    """Delete all virtual disk and unassign all hotspares physical disk
+
+    :param node: an ironic node object.
+    :param raid_controller: id of the RAID controller.
+    :returns: a dictionary containing
+              - The is_commit_required needed key with a
+              boolean value indicating whether a config job must be created
+              for the values to be applied.
+              - The is_reboot_required key with a RebootRequired enumerated
+              value indicating whether the server must be rebooted to
+              reset configuration.
+    :raises: DracOperationError on an error from python-dracclient.
+    """
+    try:
+
+        drac_job.validate_job_queue(node)
+
+        client = drac_common.get_drac_client(node)
+        return client.reset_raid_config(raid_controller)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('DRAC driver failed to delete all virtual disk '
+                  'and unassign all hotspares '
+                  'on %(raid_controller_fqdd)s '
+                  'for node %(node_uuid)s. '
+                  'Reason: %(error)s.',
+                  {'raid_controller_fqdd': raid_controller,
+                   'node_uuid': node.uuid,
+                   'error': exc})
+        raise exception.DracOperationError(error=exc)
+
+
+def clear_foreign_config(node, raid_controller):
+    """Free up the foreign drives.
+
+    :param node: an ironic node object.
+    :param raid_controller: id of the RAID controller.
+    :returns: a dictionary containing
+              - The is_commit_required needed key with a
+              boolean value indicating whether a config job must be created
+              for the values to be applied.
+              - The is_reboot_required key with a RebootRequired enumerated
+              value indicating whether the server must be rebooted to
+              clear foreign configuration.
+    :raises: DracOperationError on an error from python-dracclient.
+    """
+    try:
+
+        drac_job.validate_job_queue(node)
+
+        client = drac_common.get_drac_client(node)
+        return client.clear_foreign_config(raid_controller)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('DRAC driver failed to free foreign driver '
+                  'on %(raid_controller_fqdd)s '
+                  'for node %(node_uuid)s. '
+                  'Reason: %(error)s.',
+                  {'raid_controller_fqdd': raid_controller,
+                   'node_uuid': node.uuid,
+                   'error': exc})
+        raise exception.DracOperationError(error=exc)
+
+
+def commit_config(node, raid_controller, reboot=False, realtime=False):
     """Apply all pending changes on a RAID controller.
 
     :param node: an ironic node object.
@@ -215,7 +307,9 @@
     client = drac_common.get_drac_client(node)
 
     try:
-        return client.commit_pending_raid_changes(raid_controller, reboot)
+        return client.commit_pending_raid_changes(raid_controller=raid_controller,
+                                                  reboot=reboot,
+                                                  realtime=realtime)
     except drac_exceptions.BaseClientException as exc:
         LOG.error('DRAC driver failed to commit pending RAID config for'
                   ' controller %(raid_controller_fqdd)s on node '
@@ -630,27 +724,69 @@
     return filtered_disks
 
 
-def _commit_to_controllers(node, controllers):
-    """Commit changes to RAID controllers on the node."""
+def _commit_to_controllers(node, controllers, substep="completed"):
+    """Commit changes to RAID controllers on the node.
+
+    :param node: an ironic node object
+    :param controllers: a list of dictionary containing
+                        - The raid_controller key with raid controller
+                        fqdd value indicating on which raid configuration
+                        job needs to be perform.
+                        - The is_commit_required needed key with a
+                        boolean value indicating whether a config job must
+                        be created.
+                        - The is_reboot_required key with a RebootRequired
+                        enumerated value indicating whether the server must
+                        be rebooted only if raid controller does not support
+                        realtime.
+    :param substep: contain sub cleaning step which executes any raid
+                    configuration job if set after cleaning step.
+                    (default to completed)
+    :returns: states.CLEANWAIT if deletion is in progress asynchronously
+              or None if it is completed.
+    """
+    # remove controller which does not require configuration job
+    controllers = [controller for controller in controllers
+                   if controller['is_commit_required']]
 
     if not controllers:
         LOG.debug('No changes on any of the controllers on node %s',
                   node.uuid)
+        driver_internal_info = node.driver_internal_info
+        driver_internal_info['raid_config_substep'] = substep
+        driver_internal_info['raid_config_parameters'] = []
+        node.driver_internal_info = driver_internal_info
+        node.save()
         return
 
     driver_internal_info = node.driver_internal_info
+    driver_internal_info['raid_config_substep'] = substep
+    driver_internal_info['raid_config_parameters'] = []
+
     if 'raid_config_job_ids' not in driver_internal_info:
         driver_internal_info['raid_config_job_ids'] = []
 
-    controllers = list(controllers)
+    all_realtime = True
+    optional = drac_constants.RebootRequired.optional
     for controller in controllers:
-        # Do a reboot only for the last controller
+        raid_controller = controller['raid_controller']
+
+        # Commit the configuration
+        # The logic below will reboot the node if there is at least one
+        # controller without real time support. In that case the reboot
+        # is triggered when the configuration is committed to the last
+        # controller.
+
+        realtime = controller['is_reboot_required'] == optional
+        all_realtime = all_realtime and realtime
         if controller == controllers[-1]:
-            job_id = commit_config(node, raid_controller=controller,
-                                   reboot=True)
+            job_id = commit_config(node, raid_controller=raid_controller,
+                                   reboot=not all_realtime,
+                                   realtime=realtime)
         else:
-            job_id = commit_config(node, raid_controller=controller,
-                                   reboot=False)
+            job_id = commit_config(node, raid_controller=raid_controller,
+                                   reboot=False,
+                                   realtime=realtime)
 
         LOG.info('Change has been committed to RAID controller '
                  '%(controller)s on node %(node)s. '
@@ -660,6 +796,11 @@
 
         driver_internal_info['raid_config_job_ids'].append(job_id)
 
+        if raid_controller not in driver_internal_info[
+                'raid_config_parameters']:
+            driver_internal_info['raid_config_parameters'].append(
+                raid_controller)
+
     node.driver_internal_info = driver_internal_info
     node.save()
 
@@ -735,10 +876,10 @@
         logical_disks_to_create = _filter_logical_disks(
             logical_disks, create_root_volume, create_nonroot_volumes)
 
-        controllers = set()
+        controllers = list()
         for logical_disk in logical_disks_to_create:
-            controllers.add(logical_disk['controller'])
-            create_virtual_disk(
+            controller = dict()
+            controller_cap = create_virtual_disk(
                 node,
                 raid_controller=logical_disk['controller'],
                 physical_disks=logical_disk['physical_disks'],
@@ -747,8 +888,15 @@
                 disk_name=logical_disk.get('name'),
                 span_length=logical_disk.get('span_length'),
                 span_depth=logical_disk.get('span_depth'))
+            controller['raid_controller'] = logical_disk['controller']
+            controller['is_reboot_required'] = controller_cap[
+                'is_reboot_required']
+            controller['is_commit_required'] = controller_cap[
+                'is_commit_required']
+            if controller not in controllers:
+                controllers.append(controller)
 
-        return _commit_to_controllers(node, list(controllers))
+        return _commit_to_controllers(node, controllers)
 
     @METRICS.timer('DracRAID.delete_configuration')
     @base.clean_step(priority=0)
@@ -762,12 +910,21 @@
         """
         node = task.node
 
-        controllers = set()
-        for disk in list_virtual_disks(node):
-            controllers.add(disk.controller)
-            delete_virtual_disk(node, disk.id)
+        controllers = list()
+        drac_raid_controllers = list_raid_controllers(node)
+        for cntrl in drac_raid_controllers:
+            if _is_raid_controller(node, cntrl.id, drac_raid_controllers):
+                controller = dict()
+                controller_cap = _reset_raid_config(node, cntrl.id)
+                controller["raid_controller"] = cntrl.id
+                controller["is_reboot_required"] = controller_cap[
+                    "is_reboot_required"]
+                controller["is_commit_required"] = controller_cap[
+                    "is_commit_required"]
+                controllers.append(controller)
 
-        return _commit_to_controllers(node, list(controllers))
+        return _commit_to_controllers(node, controllers,
+                                      substep="delete_foreign_config")
 
     @METRICS.timer('DracRAID.get_logical_disks')
     def get_logical_disks(self, task):
@@ -840,9 +997,9 @@
         for config_job_id in raid_config_job_ids:
             config_job = drac_job.get_job(node, job_id=config_job_id)
 
-            if config_job.state == 'Completed':
+            if config_job is None or config_job.status == 'Completed':
                 finished_job_ids.append(config_job_id)
-            elif config_job.state == 'Failed':
+            elif config_job.status == 'Failed':
                 finished_job_ids.append(config_job_id)
                 self._set_raid_config_job_failure(node)
 
@@ -852,13 +1009,60 @@
         task.upgrade_lock()
         self._delete_cached_config_job_id(node, finished_job_ids)
 
-        if not node.driver_internal_info['raid_config_job_ids']:
-            if not node.driver_internal_info.get('raid_config_job_failure',
-                                                 False):
-                self._resume_cleaning(task)
+
+        if not node.driver_internal_info.get('raid_config_job_failure',
+                                             False):
+            if 'raid_config_substep' in node.driver_internal_info:
+                if node.driver_internal_info['raid_config_substep'] == \
+                        'delete_foreign_config':
+                    self._execute_cleaning_foreign_drives(task, node)
+                elif node.driver_internal_info['raid_config_substep'] == \
+                        'completed':
+                    self._complete_raid_cleaning_substep(task, node)
             else:
-                self._clear_raid_config_job_failure(node)
-                self._set_clean_failed(task, config_job)
+                self._complete_raid_cleaning_substep(task, node)
+        else:
+            self._clear_raid_substep(node)
+            self._clear_raid_config_job_failure(node)
+            self._set_clean_failed(task, config_job)
+
+    def _execute_cleaning_foreign_drives(self, task, node):
+        controllers = list()
+        jobs_required = False
+        for controller_id in node.driver_internal_info[
+                'raid_config_parameters']:
+            controller_cap = clear_foreign_config(
+                node, controller_id)
+            controller = {
+                'raid_controller': controller_id,
+                'is_reboot_required': controller_cap['is_reboot_required'],
+                'is_commit_required': controller_cap['is_commit_required']}
+
+            controllers.append(controller)
+            jobs_required = jobs_required or controller_cap[
+                'is_commit_required']
+
+        if not jobs_required:
+            LOG.info(
+                "No foreign drives detected, so "
+                "resume cleaning")
+            self._complete_raid_cleaning_substep(task, node)
+        else:
+            _commit_to_controllers(
+                node,
+                controllers,
+                substep='completed')
+
+    def _complete_raid_cleaning_substep(self, task, node):
+        self._clear_raid_substep(node)
+        self._resume_cleaning(task)
+
+    def _clear_raid_substep(self, node):
+        driver_internal_info = node.driver_internal_info
+        driver_internal_info.pop('raid_config_substep', None)
+        driver_internal_info.pop('raid_config_parameters', None)
+        node.driver_internal_info = driver_internal_info
+        node.save()
 
     def _set_raid_config_job_failure(self, node):
         driver_internal_info = node.driver_internal_info
