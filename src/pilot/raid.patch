--- raid_orignal.py	2020-01-27 14:20:46.457601298 +0000
+++ raid_new.py	2020-01-27 14:20:01.905979038 +0000
@@ -17,6 +17,8 @@
 
 import math
 
+from collections import defaultdict
+
 from futurist import periodics
 from ironic_lib import metrics_utils
 from oslo_log import log as logging
@@ -34,6 +36,7 @@
 from ironic.drivers.modules.drac import common as drac_common
 from ironic.drivers.modules.drac import job as drac_job
 
+drac_constants = importutils.try_import('dracclient.constants')
 drac_exceptions = importutils.try_import('dracclient.exceptions')
 
 LOG = logging.getLogger(__name__)
@@ -134,6 +137,34 @@
         raise exception.DracOperationError(error=exc)
 
 
+def _is_raid_controller(node, raid_controller_fqdd, raid_controllers=None):
+    """Find out if object's fqdd is for a raid controller or not
+
+    :param node: an ironic node object
+    :param raid_controller_fqdd: The object's fqdd we are testing to see
+                                 if it is a raid controller or not.
+    :param raid_controllers: A list of RAIDControllers used to check for
+                             the presence of BOSS cards.  If None, the
+                             iDRAC will be queried for the list of
+                             controllers.
+    :returns: boolean, True if the device is a RAID controller,
+              False if not.
+    """
+    client = drac_common.get_drac_client(node)
+
+    try:
+        return client.is_raid_controller(raid_controller_fqdd,
+                                         raid_controllers)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('Unable to determine if controller %(raid_controller_fqdd)s '
+                  'on node %(node_uuid)s is a RAID controller. '
+                  'Reason: %(error)s. ',
+                  {'raid_controller_fqdd': raid_controller_fqdd,
+                   'node_uuid': node.uuid, 'error': exc})
+
+        raise exception.DracOperationError(error=exc)
+
+
 def create_virtual_disk(node, raid_controller, physical_disks, raid_level,
                         size_mb, disk_name=None, span_length=None,
                         span_depth=None):
@@ -159,7 +190,14 @@
     drac_job.validate_job_queue(node)
 
     client = drac_common.get_drac_client(node)
-
+    LOG.info("Properties before creating VD")
+    LOG.info("IRONIC: raid controller : {}".format(raid_controller))
+    LOG.info("IRONIC: physical_disks  : {}".format(physical_disks))
+    LOG.info("IRONIC: raid_level      : {}".format(raid_level))
+    LOG.info("IRONIC: size_mb         : {}".format(size_mb))
+    LOG.info("IRONIC: disk_name       : {}".format(disk_name))
+    LOG.info("IRONIC: span_length     : {}".format(span_length))
+    LOG.info("IRONiC: span_depth      : {}".format(span_depth))
     try:
         return client.create_virtual_disk(raid_controller, physical_disks,
                                           raid_level, size_mb, disk_name,
@@ -202,7 +240,107 @@
         raise exception.DracOperationError(error=exc)
 
 
-def commit_config(node, raid_controller, reboot=False):
+def _reset_raid_config(node, raid_controller):
+    """Delete all virtual disk and unassign all hotspares physical disk
+
+    :param node: an ironic node object.
+    :param raid_controller: id of the RAID controller.
+    :returns: a dictionary containing
+              - The is_commit_required needed key with a
+              boolean value indicating whether a config job must be created
+              for the values to be applied.
+              - The is_reboot_required key with a RebootRequired enumerated
+              value indicating whether the server must be rebooted to
+              reset configuration.
+    :raises: DracOperationError on an error from python-dracclient.
+    """
+    try:
+
+        drac_job.validate_job_queue(node)
+
+        client = drac_common.get_drac_client(node)
+        return client.reset_raid_config(raid_controller)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('DRAC driver failed to delete all virtual disk '
+                  'and unassign all hotspares '
+                  'on %(raid_controller_fqdd)s '
+                  'for node %(node_uuid)s. '
+                  'Reason: %(error)s.',
+                  {'raid_controller_fqdd': raid_controller,
+                   'node_uuid': node.uuid,
+                   'error': exc})
+        raise exception.DracOperationError(error=exc)
+
+
+def clear_foreign_config(node, raid_controller):
+    """Free up the foreign drives.
+
+    :param node: an ironic node object.
+    :param raid_controller: id of the RAID controller.
+    :returns: a dictionary containing
+              - The is_commit_required needed key with a
+              boolean value indicating whether a config job must be created
+              for the values to be applied.
+              - The is_reboot_required key with a RebootRequired enumerated
+              value indicating whether the server must be rebooted to
+              clear foreign configuration.
+    :raises: DracOperationError on an error from python-dracclient.
+    """
+    try:
+
+        drac_job.validate_job_queue(node)
+
+        client = drac_common.get_drac_client(node)
+        return client.clear_foreign_config(raid_controller)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('DRAC driver failed to free foreign driver '
+                  'on %(raid_controller_fqdd)s '
+                  'for node %(node_uuid)s. '
+                  'Reason: %(error)s.',
+                  {'raid_controller_fqdd': raid_controller,
+                   'node_uuid': node.uuid,
+                   'error': exc})
+        raise exception.DracOperationError(error=exc)
+
+
+def change_physical_disk_state(node, mode=None,
+                               controllers_to_physical_disk_ids=None):
+    """Convert disks RAID status
+
+    :param node: an ironic node object.
+    :param mode: enumeration that indicates the mode
+                 to change the disks to.
+    :return: a dictionary containing:
+             - conversion_results, a dictionary that maps controller ids
+             to the conversion results for that controller.
+             The conversion results are a dict that contains:
+             - The is_commit_required key with the value always set to
+             True indicating that a config job must be created to
+             complete disk conversion.
+             - The is_reboot_required key with a RebootRequired
+             enumerated value indicating whether the server must be
+             rebooted to complete disk conversion.
+    :raises: DRACOperationFailed on error reported back by the DRAC and the
+             exception message does not contain NOT_SUPPORTED_MSG constant.
+    """
+    try:
+
+        drac_job.validate_job_queue(node)
+        client = drac_common.get_drac_client(node)
+        return client.change_physical_disk_state(
+            mode, controllers_to_physical_disk_ids)
+    except drac_exceptions.BaseClientException as exc:
+        LOG.error('DRAC driver failed to change physical drives '
+                  'in %(mode)s mode '
+                  'for node %(node_uuid)s. '
+                  'Reason: %(error)s.',
+                  {'mode': mode,
+                   'node_uuid': node.uuid,
+                   'error': exc})
+        raise exception.DracOperationError(error=exc)
+
+
+def commit_config(node, raid_controller, reboot=False, realtime=False):
     """Apply all pending changes on a RAID controller.
 
     :param node: an ironic node object.
@@ -215,7 +353,9 @@
     client = drac_common.get_drac_client(node)
 
     try:
-        return client.commit_pending_raid_changes(raid_controller, reboot)
+        return client.commit_pending_raid_changes(raid_controller=raid_controller,
+                                                  reboot=reboot,
+                                                  realtime=realtime)
     except drac_exceptions.BaseClientException as exc:
         LOG.error('DRAC driver failed to commit pending RAID config for'
                   ' controller %(raid_controller_fqdd)s on node '
@@ -485,6 +625,7 @@
 
 
 def _calculate_volume_props(logical_disk, physical_disks, free_space_mb):
+    LOG.info("logical_disk in _calculate_volume_props : ------------- : {}".format(logical_disk))
     selected_disks = [disk for disk in physical_disks
                       if disk.id in logical_disk['physical_disks']]
 
@@ -510,6 +651,7 @@
         logical_disk['raid_level'], selected_disks, free_space_mb,
         spans_count=spans_count)
 
+    LOG.info("max_volume_size_mb ------------- : {}".format(max_volume_size_mb))
     if logical_disk['size_mb'] == 'MAX':
         if max_volume_size_mb == 0:
             error_msg = _("size set to 'MAX' but could not allocate physical "
@@ -525,7 +667,7 @@
 
     disk_usage = _volume_usage_per_disk_mb(logical_disk, selected_disks,
                                            spans_count=spans_count)
-
+    LOG.info("DISK_USAGE------------- : {}".format(disk_usage))
     for disk in selected_disks:
         if free_space_mb[disk] < disk_usage:
             error_msg = _('not enough free space on physical disks for the '
@@ -630,39 +772,201 @@
     return filtered_disks
 
 
-def _commit_to_controllers(node, controllers):
-    """Commit changes to RAID controllers on the node."""
+def validate_volume_size(node, logical_disks):
+    new_physical_disks = list_physical_disks(node)
+    free_space_mb = {}
+    new_processed_volumes = []
+    for disk in new_physical_disks:
+        free_space_mb[disk] = disk.free_size_mb
+
+    for logical_disk in logical_disks:
+        current_vol_size_mb = logical_disk['size_mb']
+
+        selected_disks = [disk for disk in new_physical_disks
+                          if disk.id in logical_disk['physical_disks']]
+
+        spans_count = _calculate_spans(
+            logical_disk['raid_level'], len(selected_disks))
+
+        new_max_vol_size_mb = _max_volume_size_mb(
+            logical_disk['raid_level'],
+            selected_disks,
+            free_space_mb,
+            spans_count=spans_count)
+
+        if new_max_vol_size_mb != logical_disk['size_mb']:
+            logical_disk['size_mb'] = new_max_vol_size_mb
+            _calculate_volume_props(logical_disk, new_physical_disks, free_space_mb)
+            new_processed_volumes.append(logical_disk)
+
+    if new_processed_volumes:
+        return new_processed_volumes
+
+    return logical_disks
+
+
+def _change_physical_disk_mode(node, mode=None,
+                               controllers_to_physical_disk_ids=None,
+                               substep="completed"):
+    """physical drives conversion from raid to jbod or vice-versa.
+
+    :param node: an ironic node object.
+    :param mode: enumeration that indicates the mode
+                     to change the disks to.
+    :returns: states.CLEANWAIT if deletion is in progress asynchronously
+              or None if it is completed.
+    :raises: DRACOperationFailed on error reported back by the DRAC and the
+             exception message does not contain NOT_SUPPORTED_MSG constant.
+    """
+    change_disk_state = change_physical_disk_state(
+        node, mode, controllers_to_physical_disk_ids)
+
+    controllers = list()
+    conversion_results = change_disk_state['conversion_results']
+    for controller_id, result in conversion_results.items():
+        controller = {'raid_controller': controller_id,
+                      'is_reboot_required': result['is_reboot_required'],
+                      'is_commit_required': result['is_commit_required']}
+        controllers.append(controller)
+
+    return _commit_to_controllers(
+        node,
+        controllers, substep=substep)
+
+
+def _create_virtual_disks(task, node):
+    LOG.debug("Waiting for physical disk conversion to complete "
+              "for node %(node_uuid)s. ", {"node_uuid": node.uuid})
+    drac_job.wait_for_job_completion(node)
+
+    LOG.info(
+        "Completed converting physical disks configured to back RAID "
+        "logical disks to RAID mode for node %(node_uuid)s",
+        {'node_uuid': node.uuid})
+
+    # Check the status of physical disks and validate its volume size and properties
+    physical_disks = list_physical_disks(node)
+    logical_disks_to_create = node.driver_internal_info['logical_disks_to_create']
+
+    if node.driver_internal_info['drives_conversion']:
+        logical_disks_to_create = validate_volume_size(node, logical_disks_to_create)
+
+    controllers = list()
+    for logical_disk in logical_disks_to_create:
+        controller = dict()
+        controller_cap = create_virtual_disk(
+            node,
+            raid_controller=logical_disk['controller'],
+            physical_disks=logical_disk['physical_disks'],
+            raid_level=logical_disk['raid_level'],
+            size_mb=logical_disk['size_mb'],
+            disk_name=logical_disk.get('name'),
+            span_length=logical_disk.get('span_length'),
+            span_depth=logical_disk.get('span_depth'))
+        controller['raid_controller'] = logical_disk['controller']
+        controller['is_reboot_required'] = controller_cap[
+            'is_reboot_required']
+        controller['is_commit_required'] = controller_cap[
+            'is_commit_required']
+        if controller not in controllers:
+            controllers.append(controller)
+
+    return _commit_to_controllers(node, controllers)
+
+
+def _create_config_job(node, controller,
+                       reboot=False, realtime=False,
+                       raid_config_job_ids=[],
+                       raid_config_parameters=[]):
+    job_id = commit_config(node, raid_controller=controller,
+                           reboot=reboot,
+                           realtime=realtime)
+
+    raid_config_job_ids.append(job_id)
+    if controller not in raid_config_parameters:
+        raid_config_parameters.append(controller)
+
+    LOG.info('Change been committed to RAID controller '
+             '%(controller)s on node %(node)s. '
+             'DRAC job id: %(job_id)s',
+             {'controller': controller, 'node': node.uuid,
+              'job_id': job_id})
+    return {'raid_config_job_ids': raid_config_job_ids,
+            'raid_config_parameters': raid_config_parameters}
+
+
+def _commit_to_controllers(node, controllers, substep="completed"):
+    """Commit changes to RAID controllers on the node.
+
+    :param node: an ironic node object
+    :param controllers: a list of dictionary containing
+                        - The raid_controller key with raid controller
+                        fqdd value indicating on which raid configuration
+                        job needs to be perform.
+                        - The is_commit_required needed key with a
+                        boolean value indicating whether a config job must
+                        be created.
+                        - The is_reboot_required key with a RebootRequired
+                        enumerated value indicating whether the server must
+                        be rebooted only if raid controller does not support
+                        realtime.
+    :param substep: contain sub cleaning step which executes any raid
+                    configuration job if set after cleaning step.
+                    (default to completed)
+    :returns: states.CLEANWAIT if deletion is in progress asynchronously
+              or None if it is completed.
+    """
+    # remove controllers which does not required configuration job
+    controllers = [controller for controller in controllers
+                   if controller['is_commit_required']]
 
     if not controllers:
-        LOG.debug('No changes on any of the controllers on node %s',
-                  node.uuid)
-        return
+        driver_internal_info = node.driver_internal_info
+        driver_internal_info['raid_config_substep'] = substep
+        driver_internal_info['raid_config_parameters'] = []
+        node.driver_internal_info = driver_internal_info
+        node.save()
+        return None
 
     driver_internal_info = node.driver_internal_info
+    driver_internal_info['raid_config_substep'] = substep
+    driver_internal_info['raid_config_parameters'] = []
+
     if 'raid_config_job_ids' not in driver_internal_info:
         driver_internal_info['raid_config_job_ids'] = []
 
-    controllers = list(controllers)
-    for controller in controllers:
-        # Do a reboot only for the last controller
-        if controller == controllers[-1]:
-            job_id = commit_config(node, raid_controller=controller,
-                                   reboot=True)
-        else:
-            job_id = commit_config(node, raid_controller=controller,
-                                   reboot=False)
+    optional = drac_constants.RebootRequired.optional
+    all_realtime = all(cntlr['is_reboot_required'] == optional
+                       for cntlr in controllers)
+    raid_config_job_ids = []
+    raid_config_parameters = []
+    if all_realtime:
+        for controller in controllers:
+            realtime_controller = controller['raid_controller']
+            job_details = _create_config_job(
+                node, controller=realtime_controller,
+                reboot=False, realtime=True,
+                raid_config_job_ids=raid_config_job_ids,
+                raid_config_parameters=raid_config_parameters)
+
+    else:
+        for controller in controllers:
+            mix_controller = controller['raid_controller']
+            reboot = (controller == controllers[-1])
+            job_details = _create_config_job(
+                node, controller=mix_controller,
+                reboot=reboot, realtime=False,
+                raid_config_job_ids=raid_config_job_ids,
+                raid_config_parameters=raid_config_parameters)
 
-        LOG.info('Change has been committed to RAID controller '
-                 '%(controller)s on node %(node)s. '
-                 'DRAC job id: %(job_id)s',
-                 {'controller': controller, 'node': node.uuid,
-                  'job_id': job_id})
+    driver_internal_info['raid_config_job_ids'].extend(job_details[
+        'raid_config_job_ids'])
 
-        driver_internal_info['raid_config_job_ids'].append(job_id)
+    driver_internal_info['raid_config_parameters'].extend(job_details[
+        'raid_config_parameters'])
 
     node.driver_internal_info = driver_internal_info
     node.save()
-
     return states.CLEANWAIT
 
 
@@ -735,20 +1039,39 @@
         logical_disks_to_create = _filter_logical_disks(
             logical_disks, create_root_volume, create_nonroot_volumes)
 
-        controllers = set()
+        controllers_to_physical_disk_ids = defaultdict(list)
         for logical_disk in logical_disks_to_create:
-            controllers.add(logical_disk['controller'])
-            create_virtual_disk(
-                node,
-                raid_controller=logical_disk['controller'],
-                physical_disks=logical_disk['physical_disks'],
-                raid_level=logical_disk['raid_level'],
-                size_mb=logical_disk['size_mb'],
-                disk_name=logical_disk.get('name'),
-                span_length=logical_disk.get('span_length'),
-                span_depth=logical_disk.get('span_depth'))
+            # Not applicable to JBOD logical disks.
+            if logical_disk['raid_level'] == 'JBOD':
+                continue
 
-        return _commit_to_controllers(node, list(controllers))
+            for physical_disk_name in logical_disk['physical_disks']:
+                controllers_to_physical_disk_ids[
+                    logical_disk['controller']].append(
+                    physical_disk_name)
+
+        conversion_results = None
+        if logical_disks_to_create:
+            LOG.debug(
+                "Converting physical disks configured to back RAID "
+                "logical disks to RAID mode for node %(node_uuid)s ",
+                {"node_uuid": node.uuid})
+            raid = drac_constants.RaidStatus.raid
+            conversion_results = _change_physical_disk_mode(
+                node, raid,
+                controllers_to_physical_disk_ids,
+                substep="create_virtual_disks",)
+
+        driver_internal_info = node.driver_internal_info
+        driver_internal_info['logical_disks_to_create'] = logical_disks_to_create
+        driver_internal_info['drives_conversion'] = conversion_results
+        node.driver_internal_info = driver_internal_info
+        node.save()
+
+        if conversion_results:
+            return conversion_results
+        else:
+            return _create_virtual_disks(task, node)
 
     @METRICS.timer('DracRAID.delete_configuration')
     @base.clean_step(priority=0)
@@ -761,13 +1084,21 @@
         :raises: DracOperationError on an error from python-dracclient.
         """
         node = task.node
+        controllers = list()
+        drac_raid_controllers = list_raid_controllers(node)
+        for cntrl in drac_raid_controllers:
+            if _is_raid_controller(node, cntrl.id, drac_raid_controllers):
+                controller = dict()
+                controller_cap = _reset_raid_config(node, cntrl.id)
+                controller["raid_controller"] = cntrl.id
+                controller["is_reboot_required"] = controller_cap[
+                    "is_reboot_required"]
+                controller["is_commit_required"] = controller_cap[
+                    "is_commit_required"]
+                controllers.append(controller)
 
-        controllers = set()
-        for disk in list_virtual_disks(node):
-            controllers.add(disk.controller)
-            delete_virtual_disk(node, disk.id)
-
-        return _commit_to_controllers(node, list(controllers))
+        return _commit_to_controllers(node, controllers,
+                                      substep="delete_foreign_config")
 
     @METRICS.timer('DracRAID.get_logical_disks')
     def get_logical_disks(self, task):
@@ -836,13 +1167,12 @@
         node = task.node
         raid_config_job_ids = node.driver_internal_info['raid_config_job_ids']
         finished_job_ids = []
-
         for config_job_id in raid_config_job_ids:
             config_job = drac_job.get_job(node, job_id=config_job_id)
 
-            if config_job.state == 'Completed':
+            if config_job is None or config_job.status == 'Completed':
                 finished_job_ids.append(config_job_id)
-            elif config_job.state == 'Failed':
+            elif config_job.status == 'Failed':
                 finished_job_ids.append(config_job_id)
                 self._set_raid_config_job_failure(node)
 
@@ -851,14 +1181,79 @@
 
         task.upgrade_lock()
         self._delete_cached_config_job_id(node, finished_job_ids)
-
-        if not node.driver_internal_info['raid_config_job_ids']:
-            if not node.driver_internal_info.get('raid_config_job_failure',
-                                                 False):
-                self._resume_cleaning(task)
+        if not node.driver_internal_info.get('raid_config_job_failure',
+                                             False):
+            if 'raid_config_substep' in node.driver_internal_info:
+                if node.driver_internal_info['raid_config_substep'] == \
+                        'delete_foreign_config':
+                    foreign_drives = self._execute_foreign_drives(task, node)
+                    if foreign_drives is None:
+                        return self._convert_drives(task, node)
+                elif node.driver_internal_info['raid_config_substep'] == \
+                        'physical_disk_conversion':
+                    self._convert_drives(task, node)
+                elif node.driver_internal_info['raid_config_substep'] == \
+                        'create_virtual_disks':
+                    return _create_virtual_disks(task, node)
+                elif node.driver_internal_info['raid_config_substep'] == \
+                        'completed':
+                    self._complete_raid_substep(task, node)
             else:
-                self._clear_raid_config_job_failure(node)
-                self._set_clean_failed(task, config_job)
+                self._complete_raid_substep(task, node)
+        else:
+            self._clear_raid_substep(node)
+            self._clear_raid_config_job_failure(node)
+            self._set_clean_failed(task, config_job)
+
+    def _execute_foreign_drives(self, task, node):
+        controllers = list()
+        jobs_required = False
+        for controller_id in node.driver_internal_info[
+                'raid_config_parameters']:
+            controller_cap = clear_foreign_config(
+                node, controller_id)
+            controller = {'raid_controller': controller_id,
+                          'is_reboot_required':
+                              controller_cap[
+                                  'is_reboot_required'],
+                          'is_commit_required': controller_cap['is_commit_required']}
+            controllers.append(controller)
+            jobs_required = jobs_required or controller_cap[
+                'is_commit_required']
+
+        if not jobs_required:
+            raid_common.update_raid_info(
+                task.node, self.get_logical_disks(task))
+            LOG.info(
+                "No foreign drives detected, so "
+                "resume next substep")
+            return None
+        else:
+            return _commit_to_controllers(
+                node,
+                controllers,
+                substep='physical_disk_conversion')
+
+    def _complete_raid_substep(self, task, node):
+        self._clear_raid_substep(node)
+        self._resume_cleaning(task)
+
+    def _convert_drives(self, task, node):
+        jbod = drac_constants.RaidStatus.jbod
+        drives_results = _change_physical_disk_mode(
+            node, mode=jbod)
+        if drives_results is None:
+            LOG.debug("Controllers does not supports drives "
+                      "conversion on %(node_uuid)s",
+                      {'node_uuid': node.uuid})
+            self._complete_raid_substep(task, node)
+
+    def _clear_raid_substep(self, node):
+        driver_internal_info = node.driver_internal_info
+        driver_internal_info.pop('raid_config_substep', None)
+        driver_internal_info.pop('raid_config_parameters', None)
+        node.driver_internal_info = driver_internal_info
+        node.save()
 
     def _set_raid_config_job_failure(self, node):
         driver_internal_info = node.driver_internal_info
@@ -896,3 +1291,4 @@
         raid_common.update_raid_info(
             task.node, self.get_logical_disks(task))
         agent_base_vendor._notify_conductor_resume_clean(task)
+
